{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn as skl\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris,make_moons,make_blobs,make_regression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix,precision_score,recall_score,f1_score,\\\n",
    "    roc_curve,roc_auc_score,precision_recall_curve,accuracy_score,classification_report,\\\n",
    "    mean_squared_error,r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple linear regression problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y=make_regression(n_samples=1000, n_features=1,noise=10.0)\n",
    "\n",
    "plt.scatter(X,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=LinearRegression().fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=model.predict(X)\n",
    "\n",
    "plt.scatter(X,y,alpha=0.1)\n",
    "plt.scatter(X,y_pred,color='red',s=0.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mean squared error: {}\".format(mean_squared_error(y,y_pred)))\n",
    "print(\"R2 score: {}\".format(r2_score(y,y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rescaling the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=0.01*y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=LinearRegression().fit(X,y)\n",
    "y_pred=model.predict(X)\n",
    "\n",
    "plt.scatter(X,y,alpha=0.1)\n",
    "plt.scatter(X,y_pred,color='red',s=0.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mean squared error: {}\".format(mean_squared_error(y,y_pred)))\n",
    "print(\"R2 score: {}\".format(r2_score(y,y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Binary Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the synthetic 'moons' data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X, y = make_moons(n_samples=1000, noise=0.25, random_state=4)\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y, stratify=y)\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We look at the performance of SVM classifiers. Note that precision, recall and F1 here are maybe not so meaningful, since there is no natural 'positive' (minority) class here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_svm = SVC(kernel='rbf', gamma=0.1).fit(X_train,y_train)\n",
    "predictions = kernel_svm.predict(X_test)\n",
    "print(\"Confusion matrix:  \\n{}\\n\".format(confusion_matrix(y_test,predictions)))\n",
    "print(\"Precision: \\n{}\\n\".format(precision_score(y_test,predictions,pos_label=1)))\n",
    "print(\"Recall: \\n{}\\n\".format(recall_score(y_test,predictions,pos_label=1)))\n",
    "print(\"F1: \\n{}\".format(f1_score(y_test,predictions,pos_label=1)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now looking at the ROC curve:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fprSVM,tprSVM,thresh = roc_curve(y_test,kernel_svm.decision_function(X_test))\n",
    "plt.plot(fprSVM,tprSVM)\n",
    "print(\"Area under curve: \\n{}\".format(roc_auc_score(y_test,kernel_svm.decision_function(X_test))) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision-Recall curve:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precSVM,recSVM,thresh = precision_recall_curve(y_test,kernel_svm.decision_function(X_test))\n",
    "plt.xlabel('precision')\n",
    "plt.ylabel('recall')\n",
    "plt.plot(precSVM,recSVM)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1:** Define separate costs for false positive and false negative classifications (assume that all correct classifications have zero cost). What is the expected misclassification loss that you get from the \"default\" classifier characterized by the confustion matrix above?  What is the minimal expected cost you can get from any of the points on the ROC curve?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2 (self study):** try some other classifier(s) on this dataset, and compare their ROC/PR-curves. Do you find a classifier that strictly dominates  another in the sense that its ROC curve is always above the other? Define several different cost functions, so that different types of classifiers provide optimal solutions for different cost functions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Part 3: Multiclass Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constructing the confusion matrix from the slides from imaginary true and predicted label arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "truelabels = np.arange(220)\n",
    "truelabels[0:100]=1\n",
    "truelabels[100:110]=2\n",
    "truelabels[110:120]=3\n",
    "truelabels[120:220]=4\n",
    "predlabels = np.arange(220)\n",
    "predlabels[0:89]=1\n",
    "predlabels[89:93]=2\n",
    "predlabels[93:97]=3\n",
    "predlabels[97:100]=4\n",
    "predlabels[100:103]=2\n",
    "predlabels[103:106]=3\n",
    "predlabels[106:110]=4\n",
    "predlabels[110:112]=1\n",
    "predlabels[112:120]=3\n",
    "predlabels[120:121]=1\n",
    "predlabels[121:122]=3\n",
    "predlabels[122:220]=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Confusion matrix:  \\n{}\\n\".format(confusion_matrix(truelabels,predlabels)))\n",
    "print(\"Accuracy: \\n{}\\n\".format(accuracy_score(truelabels,predlabels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at the averaged binary scores:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"One-vs-all measures: \\n{}\\n\".format(classification_report(truelabels,predlabels)))\n",
    "print(\"Macro average F1: \\n{}\\n\".format(f1_score(truelabels,predlabels,average='macro')))\n",
    "print(\"Micro average F1: \\n{}\\n\".format(f1_score(truelabels,predlabels,average='micro')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3:** Construct two different 4x4 confusion matrices C1, C2, such that C1 has a higher accuracy score than C2, but C2 has a higher macro F1 score than C1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3 Calibration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a fairly big sample from the make_moons data generator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_moons(n_samples=30000, noise=0.25, random_state=3)\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y, stratify=y, random_state=42)\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learn a Naive Bayes and a Neural network model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes=[10],activation='tanh',solver='lbfgs', random_state=0).fit(X_train, y_train)\n",
    "nb = GaussianNB().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct histograms showing the distribution of probability predictions for the positive class. Histograms that are more concentrated at the extreme ends represent classifiers that are more 'confident' in their predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(mlp.predict_proba(X_test))\n",
    "nnposprobas=mlp.predict_proba(X_test)[:,1]\n",
    "nbposprobas=nb.predict_proba(X_test)[:,1]\n",
    "pddf = pd.DataFrame({'NN' : nnposprobas, 'NB' : nbposprobas})\n",
    "pddf.plot.hist(bins=20,alpha=0.4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This distribution says nothing about calibration. For that we create a graph that plots the value *b* of the predicted probability for the positive class against the proportion of actually positive datapoints in the small interval (*b*,*b+binwidth*). We also plot a relative measure for how many datapoints fall into each bin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# posprobas=nbposprobas\n",
    "# posprobas=nnposprobas\n",
    "\n",
    "titles=[\"Naive Bayes\",\"Multi layer perceptron\"]\n",
    "\n",
    "for j,posprobas in enumerate([nbposprobas,nnposprobas]):\n",
    "\n",
    "    binwidth=0.05\n",
    "    bins = np.arange(0,1,binwidth)\n",
    "    predperc = np.zeros(bins.size)\n",
    "    binexamples = np.zeros(bins.size)\n",
    "\n",
    "    for i,b in enumerate(bins):\n",
    "        preds = y_test[(posprobas >= b) & (posprobas < b+binwidth) ]\n",
    "        predperc[i] = np.sum(preds)/preds.size\n",
    "        binexamples[i]=preds.size\n",
    "\n",
    "    binexamples*=1/np.max(binexamples)\n",
    "\n",
    "    plt.plot(bins,predperc,c='r')\n",
    "    plt.plot(bins,binexamples,c='b')\n",
    "    plt.plot(bins,bins,c='black')\n",
    "    plt.title(titles[j])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the naive Bayes curve follows the diagonal more closely, and thus it seems a bit better calibrated than the neural network. However, the deviations from the diagonal in the neural network case are caused by rather few examples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
