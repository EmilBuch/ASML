{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import (confusion_matrix,precision_score,recall_score,f1_score,\n",
    "    roc_curve,roc_auc_score,precision_recall_curve,accuracy_score,classification_report)\n",
    "from sklearn.decomposition import LatentDirichletAllocation,PCA\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: preprocessing and vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a dataset containing movie reviews from the *Internet Movie Database*. For this, the data first needs to be downloaded from <a href=\"http://ai.stanford.edu/~amaas/data/sentiment/\"> here </a>. Note: this is about 220 Mb.  After uncompressing, the data is contained in a directory `aclImdb` with sub-directories `train` and `test`. In the following, replace the piece of the path that leads to the directory in which you have unpacked the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_train=load_files('/home/jaeger/Data/MovieReview/aclImdb/train',categories=['neg','pos'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(reviews_train.target_names)\n",
    "revidx = 2\n",
    "print(reviews_train.data[revidx])\n",
    "print(reviews_train.target[revidx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we create a dictionary (using training data only!):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary=CountVectorizer(min_df=0.0005, max_df=0.5).fit(reviews_train.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The dictionary contains {} entries\".format(len(dictionary.vocabulary_)))\n",
    "wrd='charming'\n",
    "\n",
    "print(\"The index of the word '{}' in the dictionary is {}\".format(wrd,dictionary.vocabulary_[wrd]))\n",
    "widx=708\n",
    "print(\"The word at index {} is '{}'\".format(widx,dictionary.get_feature_names_out()[widx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the underlying preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze = dictionary.build_analyzer()\n",
    "print(analyze(reviews_train.data[revidx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dictionary construction with `CountVectorizer` does not include stemming:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dictionary.get_feature_names_out()[6951:6956])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More specialized tools for tokenization, stemming etc. are provided by the 'nltk' package. We leave this out for now, and go ahead with transforming the data into tf feature vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_train_tf=dictionary.transform(reviews_train.data)\n",
    "\n",
    "print(\"The type of 'reviews_train_tf' is {} with {} rows \\\n",
    "and {} columns\".format(type(reviews_train_tf),reviews_train_tf.shape[0],reviews_train_tf.shape[1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sparse matrix structure becomes visible, when we print the first 1000 entries of the row for the review at index `revidx`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(reviews_train_tf[revidx,0:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The missing 0s are still 'there':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(reviews_train_tf[revidx,405])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also construct a tf-idf representation of the reviews:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_transformer_train=TfidfTransformer().fit(reviews_train_tf)\n",
    "reviews_train_tfidf=tfidf_transformer_train.transform(reviews_train_tf)\n",
    "print(reviews_train_tfidf[revidx,0:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the feature vector for prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning a Naive Bayes model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb=MultinomialNB().fit(reviews_train_tf,reviews_train.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading and transforming the test data. The dictionary used for the test data is the one constructed from the training data! Also the transformation with the idf values is done using the TfidfTransformer constructed from the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_test=load_files('/home/jaeger/Data/MovieReview/aclImdb/test',categories=['neg','pos'])\n",
    "reviews_test_tf=dictionary.transform(reviews_test.data)\n",
    "reviews_test_tfidf = tfidf_transformer_train.transform(reviews_test_tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying the learned model to the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_train=mnb.predict(reviews_train_tf)\n",
    "predictions_test=mnb.predict(reviews_test_tf)\n",
    "\n",
    "print(\"Accuracy on test: \\n {}\\n\".format(accuracy_score(reviews_test.target,predictions_test)))\n",
    "print(\"Accuracy on train: \\n {}\\n\".format(accuracy_score(reviews_train.target,predictions_train)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can find out about what words are most indicative of postive/negative reviews, by looking at the parameters of the model.\n",
    "\n",
    "The attribute `feature_log_prob_` returns the log probabilities of the different features (words) under the two classes. By taking the difference for the two classes, we get a measure for how much a word discriminates between the two classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_prob_diff=mnb.feature_log_prob_[0,:]-mnb.feature_log_prob_[1,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use `np.argsort` to obtain the indices of the values in log_prob_diff in increasing order:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_idxs=np.argsort(log_prob_diff)\n",
    "print(sorted_idxs[0:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and retrieve the words corresponding to these indices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numfeats=20\n",
    "print(\"The {} words most discriminating for positive reviews are:\\n\".format(numfeats))\n",
    "for i in sorted_idxs[0:numfeats]:\n",
    "    print(dictionary.get_feature_names_out()[i])\n",
    "print(\"\\n\")    \n",
    "print(\"The {} words most discriminating for negative reviews are:\\n\".format(numfeats))\n",
    "for i in sorted_idxs[len(sorted_idxs)-numfeats:len(sorted_idxs)]:\n",
    "    print(dictionary.get_feature_names_out()[i])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This indicates that we are overfitting due to very rare words. We can inspect instead words that are a little bit away from the extreme ends of the sorted log_prob_diff vector: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "numfeats=20\n",
    "offset = 500\n",
    "print(\"Words with 'positive rank' between {} and {}:\\n\".format(offset,offset+numfeats))\n",
    "for i in sorted_idxs[offset:offset+numfeats]:\n",
    "    print(dictionary.get_feature_names_out()[i])\n",
    "print(\"\\n\")    \n",
    "print(\"Words with 'positive rank' between {} and {}:\\n\".format(len(sorted_idxs)-numfeats-offset,len(sorted_idxs)-offset))\n",
    "for i in sorted_idxs[len(sorted_idxs)-numfeats-offset:len(sorted_idxs)-offset]:\n",
    "    print(dictionary.get_feature_names_out()[i])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We try a neural network classifier on the tf-idf transformed data next. We use a fairly strong regularization with `alpha=0.5` to compensate for the strong overfitting opportunities in this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "mlp = MLPClassifier(alpha=0.5).fit(reviews_train_tfidf,reviews_train.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While this is runnig (~ 5 minutes ...?), we can take a little break!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_train=mlp.predict(reviews_train_tfidf)\n",
    "predictions_test=mlp.predict(reviews_test_tfidf)\n",
    "\n",
    "\n",
    "print(\"Accuracy on test: \\n {}\\n\".format(accuracy_score(reviews_test.target,predictions_test)))\n",
    "print(\"Accuracy on train: \\n {}\\n\".format(accuracy_score(reviews_train.target,predictions_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find the reviews that are evaluated as most positive (negative) by the MLP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_positive=np.argsort( mlp.predict_proba(reviews_test_tfidf)[:,0] )\n",
    "print(most_positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "numrevs = 3\n",
    "\n",
    "print(\"The {} reviews ranked as most positive are:\\n\".format(numrevs))\n",
    "for i in most_positive[0:numrevs]:\n",
    "    print(reviews_test.data[i])\n",
    "    print(\"\\n\\n\")\n",
    " \n",
    "print(\"The {} reviews ranked as most negative are:\\n\".format(numrevs))\n",
    "for i in most_positive[len(most_positive)-numrevs:len(most_positive)]:\n",
    "    print(reviews_test.data[i])\n",
    "    print(\"\\n\\n\")\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: install the 'nltk' package https://www.nltk.org/. Now "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt') #Provides tokenization rules for English\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "should work. The package provides customizable methods for tokenization and stemming:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "revidx = 2\n",
    "\n",
    "\n",
    "#print(\"Tokenized: \\n {}\".format(tokens))\n",
    "ps=PorterStemmer()\n",
    "def stem(review):\n",
    "    tokens=word_tokenize(str(review))\n",
    "    stemmed=[]\n",
    "    for t in tokens:\n",
    "        stemmed.append(ps.stem(t))\n",
    "    return stemmed\n",
    "\n",
    "print(\"Stemmed: \\n {}\".format(stemmed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise continued**: Create a new (smaller) dictionary from stemmed versions of the reviews. How has the size of the dictionary changed, when you use the same min_df, max_df parameters in the CountVectorizer as before? Redo the NaiveBayes and MLP training with the modified data. Does accuracy improve?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmed_corpus = []\n",
    "for i in range(50):\n",
    "    stemmed_corpus.append(stem(reviews_train.data[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary=CountVectorizer(min_df=0.0005, max_df=0.5).fit(stemmed_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: word vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import a pre-computed 100-dimensional embedding of a vocabulary of 400.000 words. The data can be imported from <a href=\"http://nlp.stanford.edu/data/glove.6B.zip\">here</a> from the <a href=\"https://nlp.stanford.edu/projects/glove/\">Glove homepage</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This piece of code copied from https://stackoverflow.com/questions/37793118/load-pretrained-glove-vectors-in-python\n",
    "\n",
    "def loadGloveModel(File):\n",
    "    print(\"Loading Glove Model\")\n",
    "    f = open(File,'r')\n",
    "    gloveModel = {}\n",
    "    for line in f:\n",
    "        splitLines = line.split()\n",
    "        word = splitLines[0]\n",
    "        wordEmbedding = np.array([float(value) for value in splitLines[1:]])\n",
    "        gloveModel[word] = wordEmbedding\n",
    "    print(len(gloveModel),\" words loaded!\")\n",
    "    return gloveModel\n",
    "\n",
    "gl=loadGloveModel('/home/jaeger/Data/Glove/glove.6B.100d.txt')\n",
    "\n",
    "print(\"The type of the loaded model is {} \\n \".format(type(gl)))\n",
    "\n",
    "print(\"The embedding vector of 'university' is \\n {}\".format(gl['university']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coming back to our movie reviews, we create another dictionary from the reviews in the training set. We now set limits that terms included in the dictionary should appear in at least 5% and at most 80% of reviews. These are rather narrow bounds, which are set under the assumption that typical words expressing sentiments will fall into these bounds, while stop words and very many more specific words will be excluded. As it turns out, we create a rather small vocabulary in this manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_small=CountVectorizer(min_df=0.05, max_df=0.8).fit(reviews_train.data)\n",
    "\n",
    "ld=len(dictionary_small.vocabulary_)\n",
    "print(\"Created a dictionary with {} tokens: \\n\".format(ld)) \n",
    "for i in range(ld):\n",
    "    print(dictionary_small.get_feature_names()[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating new tf feature vectors for the smaller dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_train_tf=dictionary_small.transform(reviews_train.data)\n",
    "reviews_test_tf=dictionary_small.transform(reviews_test.data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only need the Glove vector representations of the words in our dictionary. We therefore extract from our 400.000 word original Glove dictionary the relevant ones, and collect them in a matrix:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_glove=np.zeros((ld,100))\n",
    "for i in range(ld):\n",
    "    dict_glove[i,:]=gl[dictionary_small.get_feature_names()[i]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now represent each review by the average of the Glove word vectors of the words in the review (only taking into account those that are in our dictionary) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_train_glove= np.matmul(reviews_train_tf.todense(),dict_glove)\n",
    "for r in range(25000):\n",
    "    reviews_train_glove[r,:]*=(1/np.sum(reviews_train_tf[r,:]))\n",
    "\n",
    "reviews_test_glove= np.matmul(reviews_test_tf.todense(),dict_glove)\n",
    "for r in range(25000):\n",
    "    reviews_test_glove[r,:]*=(1/np.sum(reviews_test_tf[r,:]))    \n",
    "    \n",
    "    \n",
    "print(\"glove feature matrix for training reviews has dimensions {}\".format(reviews_train_glove.shape))\n",
    "print(\"glove feature matrix for testing reviews has dimensions {}\".format(reviews_test_glove.shape))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now train classifiers for predicting the sentiment of a review from either the term-frequency features, or the GLove average embedding vectors. Remember that the tf-vectors are now over a smaller dictionary, so the accuracy results we get can be expected to be lower than in Part 1. The true class labels are retrieved by `reviews_train.target` and `reviews_test.target`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifyer = MLPClassifier((10))\n",
    "#classifyer =  LogisticRegression(solver='lbfgs')\n",
    "train={}\n",
    "train[\"tf\"]=reviews_train_tf\n",
    "train[\"glove\"]=reviews_train_glove\n",
    "test={}\n",
    "test[\"tf\"]=reviews_test_tf\n",
    "test[\"glove\"]=reviews_test_glove\n",
    "\n",
    "for features in (\"tf\",\"glove\"):\n",
    "    print(\"Training with \" + features + \"  features \\n\")\n",
    "    classifyer.fit(train[features],reviews_train.target)\n",
    "    pred_train=classifyer.predict(train[features])\n",
    "    pred_test=classifyer.predict(test[features])\n",
    "    print(\"Evaluation on training data: \\n\")\n",
    "    print(\"Confusion matrix: \\n {}\".format(confusion_matrix(pred_train,reviews_train.target)))\n",
    "    print(\"Accuracy: \\n {} \\n\".format(accuracy_score(pred_train,reviews_train.target)))\n",
    "    print(\"Evaluation on test data: \\n\")\n",
    "    print(\"Confusion matrix: \\n {}\".format(confusion_matrix(pred_test,reviews_test.target)))\n",
    "    print(\"Accuracy: \\n {}\\n\\n\".format(accuracy_score(pred_test,reviews_test.target)))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dependent on the random initializations, some learning runs here may end with an warning message that the optimization has not converged after the maximum number of iterations. In that case one can re-run, increase the maximum number of iterations, or live with the results that one has obtained from the not fully converged runs...\n",
    "\n",
    "In these initial experiments, we are likely to see results where the Glove features lead to a slightly lower accuracy on the test data than the TF features. One could explore this further in many different dimensions: modify the underlying vocabulary, create more sophisticated Glove-based features than simple averages, consider other classification models, ...\n",
    "\n",
    "Finally, we can visualize the Glove embeddings of the words in our dictionary by plotting a 2-dimensional projection computed by principal component analysis (PCA): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca=PCA(n_components=2)\n",
    "glove_pca=pca.fit_transform(dict_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30,30))\n",
    "plt.scatter(glove_pca[:,0],glove_pca[:,1],)\n",
    "\n",
    "for i in range(ld):\n",
    "    plt.annotate(dictionary_small.get_feature_names()[i],(glove_pca[i,0],glove_pca[i,1]),fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This picture can partly explain why our sentiment analysis task remains hard with Glove features:  relevant descriptive adjectives 'stupid', 'awful', 'wonderful', 'funny', 'boring', ... are clustered in the same area of the latent space. It would be more helpful, if the positive adjectives were well separated from the negative adjectives (bearing in mind, that this 2-d projection does not tell the whole story!). \n",
    "\n",
    "An interesting detail can be seen on the top of the plot: from this embedding, we would obtain for the analogical query <i>'films' is to 'film' what ??? is to 'movie'</i> the answer ???='movies'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since LDA requires some rather time-consuming computations, we again create a dictionary with a suitable size ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary=CountVectorizer(min_df=0.01, max_df=0.15).fit(reviews_train.data)\n",
    "print(\"The dictionary contains {} entries\".format(len(dictionary.vocabulary_)))\n",
    "reviews_train_tf=dictionary.transform(reviews_train.data)\n",
    "reviews_train_tfidf=TfidfTransformer().fit_transform(reviews_train_tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now fit an LDA model with `ntopics` latent topics. The LDA learner has a parameter `max_iter` that determines how many iterations over the whole dataset are performed. A *perplexity* score measures how well the learned model explains/fits the data. When time permits, one can see how the perplexity score improves when one allows more iterations in the learning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntopics=20\n",
    "\n",
    "#maxiters = np.array([5,10])\n",
    "maxiters = 5\n",
    "\n",
    "for m in range(maxiters):\n",
    "    start=time.time()\n",
    "    lda = LatentDirichletAllocation(n_components=ntopics,learning_method='online',max_iter=m).fit(reviews_train_tf)\n",
    "    end=time.time()\n",
    "    print(\"Time: {}s\".format(end-start))\n",
    "    print(\"Iterations performed: {}\".format(lda.n_iter_))\n",
    "    print(\"Perplexity score: {}\".format(lda.perplexity(reviews_train_vec)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The attribute `components_` contains the word probabilities for the latent topics (not strictly probabilities, because they do not sum to 1 over all words):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lda.components_.shape)\n",
    "comp=0\n",
    "f=30\n",
    "print(\"The first {} words have the following 'probabilities' in component {}:\\n {}\\n\"\\\n",
    "      .format(f,comp,lda.components_[comp,0:f]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next investigate which words have the highest probabilities under the different topics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "widx_ranks=np.argsort(lda.components_,axis=1)\n",
    "\n",
    "numwords=20\n",
    "for i in np.arange(ntopics):\n",
    "    print(\"Topic {}\\n\".format(i))\n",
    "    for j in np.arange(numwords):\n",
    "        print(dictionary.get_feature_names()[widx_ranks[i,len(dictionary.vocabulary_)-j-1]])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we investigate the topic distributions assigned to the different reviews. These have first to be computed using the `transform` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topicvecs = lda.transform(reviews_train_tf)\n",
    "\n",
    "print(topicvecs.shape)\n",
    "print(topicvecs[0:3,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can look which reviews are most strongly connected to any specific topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "toprevs_by_topic=np.argsort(topicvecs,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The index of the topic we want to investigate\n",
    "topicno=7\n",
    "# The number of reviews most highly associated with the topic that we want to see\n",
    "norevs=4\n",
    "\n",
    "l=len(toprevs_by_topic)-1\n",
    "\n",
    "for i in np.arange(norevs):    \n",
    "    print(reviews_train.data[toprevs_by_topic[l-i,topicno]])\n",
    "\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Try different classifiers (MLP, SVM) for classifying the reviews based on their topic vectors. <b>\n",
    "    \n",
    "**Exercise (self study):** Fix a dimensionality d for vector representations, e.g. d=50, d=100, d=1000. Construct different representations of the reviews as d-dimensional vectors: <b>\n",
    "    \n",
    "<ul>\n",
    "    <li>tf-idf vectors for a dictionary of size d</li>\n",
    "     <li>mean word embedding vectors from d-dimensional word vectors (the GloVe homepage provides embeddings of different dimensions</li>\n",
    "    <li>LDA topic vectors for d topics </li>\n",
    " </ul>\n",
    "    \n",
    "Train and evaluate different classifiers on these different representations. What are the relevant tuning possibilities you can use to optimize the performance of each type of representation? Which approach gives you the best performance for the given \"budget\" of d dimensions for representation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
