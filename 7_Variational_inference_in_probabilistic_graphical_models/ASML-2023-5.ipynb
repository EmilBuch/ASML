{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Simple Gaussian Model with BBVI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# Generate data from a simple model: Normal(10, 1)\n",
    "data = np.random.normal(loc = 10, scale = 1, size = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model specification\n",
    "\n",
    "\n",
    "<img src=\"mean_model.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual estimation of the gradient of the ELBO for the above model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient estimator using sampling -- vanilla BBVI\n",
    "# We here assume the model X ~ Normal(mu, 1)\n",
    "# with unknown mu, that in itself is Normal, mean 0 and standard deviation 1000, \n",
    "# so effectively an uniformed prior. \n",
    "# The variational dstribution for mu is also Normal, with parameter q_mu_lambda\n",
    "# -- taking the role of lambda in the calculations -- and variance 1.\n",
    "\n",
    "def grad_estimate(q_mu_lambda, samples = 1):\n",
    "    # sum_grad_estimate will hold the sum as we move along over the <samples> samples. \n",
    "    sum_grad_estimate = 0\n",
    "    for i in range(samples):\n",
    "        # Sample one example from current best guess for the variational distribution\n",
    "        mu_sample = np.random.normal(loc=q_mu_lambda, scale=1, size=1)\n",
    "        \n",
    "        # Now we want to calculate the contribution from this sample, namely \n",
    "        # [log p(x, mu_sample) - log q(mu|lambda) ] * grad( log q(mu_sample|lambda) )\n",
    "        #\n",
    "        # First log p(x|mu_sample) + log p(mu_sample) - log q(mu_sample|lambda) \n",
    "        value = np.sum(norm.logpdf(data, loc=mu_sample, scale=1)) \n",
    "        + norm.logpdf(mu_sample, loc = 0, scale = 1000)  \n",
    "        - norm.logpdf(mu_sample, loc= q_mu_lambda, scale = 1)\n",
    "        \n",
    "        # Next grad (log q(mu_sample|lambda))\n",
    "        # The Normal distribution gives the score function with known variance as <value> - <mean>\n",
    "        grad_q = mu_sample - q_mu_lambda\n",
    "        \n",
    "        # grad ELBO for this sample is therefore in total given by\n",
    "        sum_grad_estimate = sum_grad_estimate + grad_q * value\n",
    "        \n",
    "    # Divide by number of samples to get average value -- the estimated expectation  \n",
    "    return sum_grad_estimate/samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking the variation in gradient estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To check the variation / \"unreliability\" of the gradient estimate we repeat \n",
    "# several times for the same lambda value and notice difference\n",
    "\n",
    "# Location to check -- close to the data mean (at +10). \n",
    "# The prior will move the variational optimium **slightly** away from the data mean, \n",
    "# but due to the large prior variance of mu this should be a very limited effect.\n",
    "# We should therefore expect a positive derivative (since we want to move \n",
    "# q_mu_lambda towards the data mean, that is, **increase** it)\n",
    "q_mu_lambda = 9\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.set()\n",
    "grad_est = {}\n",
    "# Do with different sample sizes\n",
    "for sample_count in [1, 2, 3, 4, 5, 10, 25]:\n",
    "\n",
    "    #loop\n",
    "    q_grad = []\n",
    "    for t in range(500):\n",
    "        q_grad.append(grad_estimate(q_mu_lambda, samples=sample_count)[0])\n",
    " \n",
    "    grad_est[\"$M = {:d}$\".format(sample_count)] = q_grad\n",
    "    \n",
    "    # Report back\n",
    "    print(\"M = {:2d} sample(s) in BBVI -- Mean of gradient: {:7.3f}; Std.dev. of gradient: {:7.3f}\".format(\n",
    "        sample_count, np.mean(q_grad), np.std(q_grad)))\n",
    "\n",
    "df = pd.DataFrame(grad_est)\n",
    "sns.kdeplot(data=df)    \n",
    "plt.xlim([-500, 500])\n",
    "plt.show()      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* What can we say about the gradient estimate based on the plots above?\n",
    "* Implement and test your own gradient ascent learning algorithm starting from, say, q_mu_lambda=-10. Experiment with different learning rates, number of samples used for the gradient estimate, possibly supplemented with momentum.\n",
    "* Optional: Adapt the implementation above so that we can also learn the variance of the q distribution "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Thumb tack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyro\n",
    "import torch\n",
    "from pyro.optim import SGD, Adam\n",
    "import pyro.distributions as dist\n",
    "from torch.distributions import constraints\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import beta\n",
    "%matplotlib inline\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "We consider the thumb tack model (to see the figures you should be logged in to Moodle):\n",
    "\n",
    "<img src=\"thumb_tack.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall the beta distribution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = [(1,1), (2,2), (4,1),(2,5)]\n",
    "x = np.linspace(0,1,1000)\n",
    "plt.figure(figsize=(15, 5))\n",
    "for idx, para in enumerate(parameters):\n",
    "    plt.subplot(1, len(parameters), idx+1)\n",
    "    y = beta.pdf(x, *para)\n",
    "    plt.title(f'a={para[0]},b={para[1]}')\n",
    "    plt.plot(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The model\n",
    "\n",
    "Here we define the probabilistic model. Notice the close resemblance with the plate specification above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines the thumb_tack model. The 'data' is a 0-1 tensor of type float  \n",
    "def thumb_tack_model(data):  \n",
    "    \n",
    "    # Define the random variable theta\n",
    "    theta = pyro.sample(\"theta\", dist.Beta(2.0,5.0))\n",
    "    \n",
    "    # and now the plate holding the observations. The number of observations are determined by the data set \n",
    "    # supplied to the function\n",
    "    with pyro.plate(\"thumb_tack_plate\"):\n",
    "        pyro.sample(f\"obs\", dist.Bernoulli(probs=theta), obs=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The variational distribution\n",
    "\n",
    "In Pyro the variational distribution is defined as a so-called guide. In this example our variational distribution is a beta distribution with parameters q_alpha and q_beta:\n",
    "\n",
    "$$\n",
    "q(\\theta)= \\mathit{Beta}(\\theta | \\alpha, \\beta)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def thumb_tack_guide(data):\n",
    "\n",
    "    # We initialize the variational parameters q_alpha and q_beta to 1.0. Also, we constrain the parameters to be positive as per \n",
    "    # definition of the distribution\n",
    "    q_alpha = pyro.param(\"q_alpha\", torch.tensor(1.0), constraint=constraints.positive)\n",
    "    q_beta = pyro.param(\"q_beta\", torch.tensor(1.0), constraint=constraints.positive)\n",
    "\n",
    "    # The name of the random variable of the variational distribution must match the name of the corresponding\n",
    "    # variable in the model exactly.\n",
    "    pyro.sample(\"theta\", dist.Beta(q_alpha, q_beta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning\n",
    "\n",
    "For optimizing the ELBO we rely on a standard stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def thumb_tack_learn(data, num_iter=5000):\n",
    "\n",
    "    pyro.clear_param_store()\n",
    "\n",
    "    # Define the ELBO and the optimization function\n",
    "    elbo = pyro.infer.Trace_ELBO()\n",
    "    svi = pyro.infer.SVI(model=thumb_tack_model,\n",
    "                         guide=thumb_tack_guide,\n",
    "                         optim=SGD({'lr':0.001}),\n",
    "                         loss=elbo)\n",
    "\n",
    "    # Perform a fixed number of gradient steps\n",
    "    num_steps = num_iter\n",
    "    for step in range(num_steps):\n",
    "        loss = svi.step(data)\n",
    "\n",
    "        if step % 100 == 0:\n",
    "            print(f\"Loss for iteration {step}: {loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze\n",
    "\n",
    "Let's take a look at the learned variational distribution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def thumb_tack_analyze(data):\n",
    "\n",
    "    # Get the values of the variational parameters\n",
    "    q_alpha = pyro.param(\"q_alpha\").item()\n",
    "    q_beta = pyro.param(\"q_beta\").item()\n",
    "\n",
    "    mean = q_alpha/(q_alpha + q_beta)\n",
    "    std = q_alpha*q_beta/(((q_alpha+q_beta)**2)*(q_alpha + q_beta + 1.0))\n",
    "\n",
    "    print(f\"Mean: {mean}\")\n",
    "    print(f\"Standard deviation: {std}\")\n",
    "\n",
    "    x = np.linspace(0.0, 1.0, 1000)\n",
    "    plt.plot(x, beta.pdf(x, q_alpha, q_beta), label='Variational dist.')\n",
    "    plt.plot(x,beta.pdf(x, data[data==1].shape[0]+1,data[data==0].shape[0]+1), label='True dist.')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# The data consists of 20 pin ups ('1') and 80 pin down ('0'). Squeeze is just used to compress the dimensions \n",
    "# from 2 to 1\n",
    "data = torch.cat((torch.ones(20, 1), torch.zeros(80, 1))).squeeze()\n",
    "\n",
    "# Do learning\n",
    "thumb_tack_learn(data, num_iter=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thumb_tack_analyze(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Part 3: Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this exercise we will play around with the same model as in Part 1, where we derived the required gradients manually. Here we instead rely on differentiation functionality in Pyro, which, in turn is based n PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The model in plate notation\n",
    "\n",
    "<img src=\"mean_model.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The model defined in Pyro\n",
    "\n",
    "Here we define the probabilistic model. Notice the close resemblance with the plate specification above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_model(data):\n",
    "\n",
    "    # Define the random variable mu having a noral distribution as prior\n",
    "    mu = pyro.sample(\"mu\", dist.Normal(0.0,1000.0))\n",
    "\n",
    "    # and now the plate holding the observations. The number of observations are determined by the data set \n",
    "    # supplied to the function. \n",
    "    with pyro.plate(\"x_plate\"):\n",
    "        pyro.sample(\"x\", dist.Normal(mu, 1), obs=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The variational distribution\n",
    "\n",
    "In Pyro the variational distribution is defined as a so-called guide. In this example our variational distribution is a beta distribution with parameters q_alpha and q_beta:\n",
    "\n",
    "$$\n",
    "q(\\mu)= \\mathit{Normal}(\\mu | q_{mu}, 1)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_guide(data):\n",
    "\n",
    "    # We initialize the variational parameter to 0.0. \n",
    "    q_mu = pyro.param(\"q_mu\", torch.tensor(0.0))\n",
    "\n",
    "    # The name of the random variable of the variational distribution must match the name of the corresponding\n",
    "    # variable in the model exactly.\n",
    "    pyro.sample(\"mu\", dist.Normal(q_mu, 1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning\n",
    "\n",
    "Here we encapsulate the learning steps, relying on standard stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn(data):\n",
    "\n",
    "    pyro.clear_param_store()\n",
    "\n",
    "    elbo = pyro.infer.Trace_ELBO()\n",
    "    svi = pyro.infer.SVI(model=mean_model,\n",
    "                         guide=mean_guide,\n",
    "                         optim=SGD({'lr':0.0001}),\n",
    "                         loss=elbo)\n",
    "\n",
    "    num_steps = 1000\n",
    "    for step in range(num_steps):\n",
    "        loss = svi.step(data)\n",
    "\n",
    "        if step % 50 == 0:\n",
    "            print(f\"Loss for iteration {step}: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.tensor(np.random.normal(loc=10.0, scale=1.0, size=100),dtype=torch.float)\n",
    "learn(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the learned variational parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qmu = pyro.param(\"q_mu\").item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Mean of variational distribution: {qmu}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "* Adapt the code above to accomodate a slight more rich variational distribution, where we also have a variational parameter for the standard deviation:\n",
    "$$\n",
    "q(\\mu)= \\mathit{Normal}(\\mu | q_{mu}, q_{std})\n",
    "$$\n",
    "* Experiment with different data sets and parameter values. Try visualizing the variational posterior distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Self study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this self study you will explore Pyro a bit further, but as opposed to the last self study session where focus was on modeling we will here take a slightly closer lool at (variational) inference in Pyro. \n",
    "\n",
    "Before starting on the self study, consider revisiting the Pyro documentation listed under reading material for the last two lectures:\n",
    "* http://pyro.ai/examples/intro_long.html\n",
    "* http://pyro.ai/examples/bayesian_regression.html\n",
    "* http://pyro.ai/examples/svi_part_i.html\n",
    "\n",
    "Afterwards, continue with the notebook below, where we consider (Bayesian) linear regression using Pyro based on the same setup as in the lecture. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyro\n",
    "import torch\n",
    "import matplotlib\n",
    "#matplotlib.use('TkAgg')\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pyro.distributions as dist\n",
    "import torch.distributions.constraints as constraints\n",
    "from pyro.infer import SVI, Trace_ELBO\n",
    "from pyro.optim import Adam, SGD\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate data\n",
    "This data generation is similar to what was done during the lecture; we have one predictor variable 'x' and one response variable 'y', but here collected in a  dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(N=10, true_w0= 1., true_w1=.5):\n",
    "    gamma = 4.  # The *precision* in the observation noise\n",
    "    st_dev = 1. / np.sqrt(gamma)  # And corresponding standard deviation\n",
    "    np.random.seed(123)\n",
    "    x = 5 * np.random.rand(N)  # The x-points are sampled uniformly on [0, 5]\n",
    "    y = np.random.normal(loc=true_w0 + true_w1 * x, scale=st_dev)  # And the response is sampled from the Normal\n",
    "    return {\"x\": torch.tensor(x, dtype=torch.float), \"y\": torch.tensor(y, dtype=torch.float)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function for visualizing the data as well as the true and learned functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_plotter(data, true_w0=None, true_w1=None,\n",
    "                 approx_w0=None, approx_w1=None):\n",
    "    \"\"\"\n",
    "    Use to plot data. If y is not none it contains responses, and (x,y) will be scatter-plotted\n",
    "    If neither true_w0 nor true_w1 is None, we will plot the line true_w0 + x * true_w1 in red.\n",
    "    If neither approx_w0 nor approx_w1 is None, we plot the line approx_w0 + x * approx_w1 in green.\n",
    "    \"\"\"\n",
    "    if data is not None:\n",
    "        plt.plot(data[\"x\"].numpy(), data[\"y\"].numpy(), \"bo\")\n",
    "\n",
    "    # Plot true line if given\n",
    "    if true_w0 is not None and true_w1 is not None:\n",
    "        plt.plot(data[\"x\"].numpy(), true_w0 + true_w1 * data[\"x\"].numpy(), \"r-\")\n",
    "\n",
    "    # Plot approximation if given\n",
    "    if approx_w0 is not None and approx_w1 is not None:\n",
    "        plt.plot(data[\"x\"].numpy(), approx_w0+ approx_w1* data[\"x\"].numpy(), \"g-\", alpha=.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a data set with 50 data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_w0= 1.\n",
    "true_w1=.5\n",
    "data = generate_data(N=10, true_w0=true_w0, true_w1=true_w1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the data together with the regression line around which the data has been generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_plotter(data, true_w0=true_w0, true_w1=true_w1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specification of the Pyro model and guide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we specify a (Bayesian) linear regression model in Pyro. The 'data' argument is a dictionary covering the data of the predictor and response variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lin_reg_model(data):\n",
    "\n",
    "    w0 = pyro.sample(\"w0\", dist.Normal(0.0, 1000.0))\n",
    "    w1 = pyro.sample(\"w1\", dist.Normal(0.0, 1000.0))\n",
    "\n",
    "    with pyro.plate(\"data_plate\"):\n",
    "        pyro.sample(\"y\", dist.Normal(data[\"x\"] * w1 + w0, 1.0), obs=data[\"y\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we specify the variational distribution, which is called a guide in Pyro. We make the mean field assumption and assume that the variational distribution factorizes wrt. to 'w0' and 'w1'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lin_reg_guide(data):\n",
    "    w0_mean = pyro.param(\"w0_mean\", torch.tensor(0.0))\n",
    "    w0_scale = pyro.param(\"w0_scale\", torch.tensor(1.0), constraint=constraints.positive)\n",
    "    pyro.sample(\"w0\", dist.Normal(w0_mean, w0_scale))\n",
    "    \n",
    "    w1_mean = pyro.param(\"w1_mean\", torch.tensor(0.0))\n",
    "    w1_scale = pyro.param(\"w1_scale\", torch.tensor(1.0), constraint=constraints.positive)\n",
    "    pyro.sample(\"w1\", dist.Normal(w1_mean, w1_scale))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning\n",
    "\n",
    "In this function the actual learning is taking place. Notice that the structure is similar to what we saw in the example notebooks during the lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn(N=10, data=None):\n",
    "    if data is None:\n",
    "        data = generate_data(N=N)\n",
    "\n",
    "    pyro.clear_param_store()\n",
    "\n",
    "    elbo = pyro.infer.Trace_ELBO()\n",
    "    svi = pyro.infer.SVI(model=lin_reg_model,\n",
    "                         guide=lin_reg_guide,\n",
    "                         optim=SGD({\"lr\": 0.0001}),\n",
    "                         loss=elbo)\n",
    "\n",
    "    num_steps = 5000\n",
    "    for step in range(num_steps):\n",
    "        loss = svi.step(data)\n",
    "\n",
    "        if step % 100 == 0:\n",
    "            w0_mean = pyro.param(\"w0_mean\").detach().item()\n",
    "            w0_scale = pyro.param(\"w0_scale\").detach().item()\n",
    "            w1_mean = pyro.param(\"w1_mean\").detach().item()\n",
    "            w1_scale = pyro.param(\"w1_scale\").detach().item()\n",
    "            print(f\"Loss (iter: {step}): {loss}\")\n",
    "            print(f\"w0: {w0_mean} +/- {w0_scale}\\t \\t w1: {w1_mean} +/- {w1_scale}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn(data=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze the results\n",
    "\n",
    "Here we sample weights from the posterior distributions over 'w0' and 'w1'. The distribution of the generated weights (and the corresponding models) illustrates how confident we are in the model, an insight you cannot get when only having point estimates of the model parameters as found with, e.g., maximum likelihood learning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(100):\n",
    "    w0_mean = pyro.param(\"w0_mean\").detach().item()\n",
    "    w0_scale = pyro.param(\"w0_scale\").detach().item()\n",
    "    w1_mean = pyro.param(\"w1_mean\").detach().item()\n",
    "    w1_scale = pyro.param(\"w1_scale\").detach().item()\n",
    "    w0_sample = pyro.sample(\"w0_sample\", dist.Normal(w0_mean, w0_scale)).numpy()\n",
    "    w1_sample = pyro.sample(\"w1_sample\", dist.Normal(w1_mean, w1_scale)).numpy()\n",
    "    data_plotter(data, approx_w0=w0_sample, approx_w1=w1_sample)\n",
    "data_plotter(data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises:\n",
    "* Generate data sets of varying sizes and characteristics (by changing the parameters in the 'generate_data' function) and investigate and compare the resulting models.\n",
    "* Analyze how learning is affected by changing the learning rate and the initial values of the parameters specified in the guide function.\n",
    "* Experiment with different types of prior knowledge in the model specification (e.g. change the mean and scale of the distributions over the weights). For instance, we may (mostly likely erroneously considering the data) have a prior expectation that 'w0' is around 5.0, and we can encode the strength of this belief through the scale of the corresponding distribution. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
