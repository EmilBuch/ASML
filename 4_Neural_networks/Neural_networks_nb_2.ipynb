{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ab51a06-4ee6-4819-a45f-e19e1c6989b8",
   "metadata": {},
   "source": [
    "# Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7fab82-b5fd-4ec3-ad00-58cd60cdf435",
   "metadata": {},
   "source": [
    "Revisit the Jupyter notebook from the previous self study session; there we implemented a simple neural network using functionality from numpy and PyTorch. \n",
    "\n",
    "### Exercise: \n",
    "* Modify the code by introducing regularization (L1 and/or L2). \n",
    "* How does the regularization affect the neural network results and the weights being learned? Try also experimenting with different weights for the regularization term."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7768dd-a1e5-4fd2-a9e3-c19801e0ffaa",
   "metadata": {},
   "source": [
    "# Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478031b5-5f9c-4070-80be-ca6c629bb8f6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Exercise\n",
    "\n",
    "In this exercise, the intention is to get a bit more handson experience with the convolution operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32ce2dd4-be5d-4d73-8a01-0f6b58ec72b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pathlib import Path\n",
    "import requests\n",
    "from matplotlib import pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "import gzip\n",
    "from scipy import signal\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b714a973-0b07-480c-a551-7119b6cda60a",
   "metadata": {},
   "source": [
    "The MNIST database consists of grey scale images of handwritten digits. Each image is of size $28\\times 28$; see figure below for an illustration. The data set is divided into a training set, validation set, and test set consisting of $50000$, $10000$, and $10000$ images, respectively; in all data sets the images are labeled with the correct digits. If interested, you can find more information about the MNIST data set at http://yann.lecun.com/exdb/mnist/, including accuracy results for various machine learning methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb8faa3-2a97-40b4-be4a-733fa8f1cfc8",
   "metadata": {},
   "source": [
    "First we download the dataset and unpackage it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6881a6cf-ba30-4018-8335-57af1172b77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path(\"data\")\n",
    "PATH = DATA_PATH / \"mnist\"\n",
    "\n",
    "PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "URL = \"https://github.com/pytorch/tutorials/raw/master/_static/\"\n",
    "#URL = \"http://deeplearning.net/data/mnist/\"\n",
    "FILENAME = \"mnist.pkl.gz\"\n",
    "\n",
    "if not (PATH / FILENAME).exists():\n",
    "        content = requests.get(URL + FILENAME).content\n",
    "        (PATH / FILENAME).open(\"wb\").write(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9127ce8f-06b9-4423-be70-6ae788fc4ccc",
   "metadata": {},
   "source": [
    "We then extract the data and store it numpy arrays: x_train, y_train, x_valid, y_valid, x_test, y_test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b2f724a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 20\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('../data', train=True, download=True,\n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize((0.1307,), (0.3081,))\n",
    "                       ])),\n",
    "        batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb215b29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch dimension (digit): torch.Size([20, 1, 28, 28])\n",
      "Batch dimension (target): torch.Size([20])\n",
      "Target: 7 with shape torch.Size([])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch = next(iter(train_loader))\n",
    "print(f\"Batch dimension (digit): {batch[0].shape}\")\n",
    "print(f\"Batch dimension (target): {batch[1].shape}\")\n",
    "digit_batch = batch[0]\n",
    "img = digit_batch[0,:]\n",
    "pyplot.imshow(img.reshape((28, 28)), cmap=\"gray\")\n",
    "print(f\"Target: {batch[1][0]} with shape {batch[1][0].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a504f9e9-ed17-441b-bdf6-713525bea0a5",
   "metadata": {},
   "source": [
    "Check the shape of the x_valid holding the validation data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f50004-4593-4a37-a9c4-3592573b7dd8",
   "metadata": {},
   "source": [
    "The images are stored in rows of length $784$, hence to display the images we need to reshape them to $28\\times 28$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ce92d745-28ca-4657-8ea8-52e55f4c1771",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 1, 28, 28])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pyplot.imshow(batch[0][3,:].reshape((28, 28)), cmap=\"gray\")\n",
    "print(batch[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3832adbe-4018-443b-bcaf-d81906b9d0ac",
   "metadata": {},
   "source": [
    "We can also convert the image into black and white"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3b6edfd4-3541-4864-b385-4b46ae9c4c70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAGdCAYAAABU0qcqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAYqUlEQVR4nO3df0xV9/3H8ddV4VZbuBQRLreiRW01qZVlThlxdU0giltM/fGH6/qHXYyN9tpMXbvFJWq7LGGzSbN0Mev+0iyrtjMZmprMRFEw3dCmVmPMOiKMDY1cXE04F1GvBj7fP1jvvldBBe/lfe/l+Ug+Sbn3eO+b44Fnr/dw8DnnnAAAGGXjrAcAAIxNBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJiYYD3A3fr7+3XlyhXl5eXJ5/NZjwMAGCbnnHp6ehQKhTRu3NCvc9IuQFeuXFFZWZn1GACAR3Tp0iVNnTp1yPvT7p/g8vLyrEcAACTBg76fpyxAu3fv1tNPP63HHntMlZWV+uyzzx7qz/HPbgCQHR70/TwlAfr444+1detW7dy5U1988YUqKiq0dOlSXb16NRVPBwDIRC4FFi5c6MLhcPzjvr4+FwqFXF1d3QP/rOd5ThKLxWKxMnx5nnff7/dJfwV0+/ZtnTlzRjU1NfHbxo0bp5qaGjU3N9+zfSwWUzQaTVgAgOyX9AB99dVX6uvrU0lJScLtJSUlikQi92xfV1enQCAQX5wBBwBjg/lZcNu2bZPnefF16dIl65EAAKMg6T8HVFRUpPHjx6urqyvh9q6uLgWDwXu29/v98vv9yR4DAJDmkv4KKDc3V/Pnz1dDQ0P8tv7+fjU0NKiqqirZTwcAyFApuRLC1q1btXbtWn3rW9/SwoUL9Zvf/Ea9vb360Y9+lIqnAwBkoJQEaM2aNfrPf/6jHTt2KBKJ6Bvf+IaOHDlyz4kJAICxy+ecc9ZD/H/RaFSBQMB6DADAI/I8T/n5+UPeb34WHABgbCJAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACaSHqC3335bPp8vYc2ZMyfZTwMAyHATUvGgzz33nI4dO/a/J5mQkqcBAGSwlJRhwoQJCgaDqXhoAECWSMl7QBcvXlQoFNKMGTP0yiuvqKOjY8htY7GYotFowgIAZL+kB6iyslJ79+7VkSNH9Lvf/U7t7e164YUX1NPTM+j2dXV1CgQC8VVWVpbskQAAacjnnHOpfILu7m5Nnz5d7733ntatW3fP/bFYTLFYLP5xNBolQgCQBTzPU35+/pD3p/zsgIKCAj377LNqbW0d9H6/3y+/35/qMQAAaSblPwd0/fp1tbW1qbS0NNVPBQDIIEkP0Jtvvqmmpib961//0t/+9jetXLlS48eP18svv5zspwIAZLCk/xPc5cuX9fLLL+vatWuaMmWKvvOd7+jUqVOaMmVKsp8KAJDBUn4SwnBFo1EFAgHrMcaUNDsEksLn81mPAIx5DzoJgWvBAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmUv4L6QAL6X6BVS6WCvAKCABghAABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACa4GjZGfGXmdL/idDpL933H1boxGngFBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCY4GKkGLGRXLAy3S/CiQEj+XviAqYYLl4BAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmuBgpRlU2XrCSC6wCI8MrIACACQIEADAx7ACdPHlSy5cvVygUks/n08GDBxPud85px44dKi0t1cSJE1VTU6OLFy8ma14AQJYYdoB6e3tVUVGh3bt3D3r/rl279P777+uDDz7Q6dOn9fjjj2vp0qW6devWIw8LAMgi7hFIcvX19fGP+/v7XTAYdO+++278tu7ubuf3+93+/fsf6jE9z3OSWKyMWRhg/ffASr/led59j5mkvgfU3t6uSCSimpqa+G2BQECVlZVqbm4e9M/EYjFFo9GEBQDIfkkNUCQSkSSVlJQk3F5SUhK/7251dXUKBALxVVZWlsyRAABpyvwsuG3btsnzvPi6dOmS9UgAgFGQ1AAFg0FJUldXV8LtXV1d8fvu5vf7lZ+fn7AAANkvqQEqLy9XMBhUQ0ND/LZoNKrTp0+rqqoqmU8FAMhww74Uz/Xr19Xa2hr/uL29XefOnVNhYaGmTZumzZs365e//KWeeeYZlZeXa/v27QqFQlqxYkUy5wYAZLrhnmp54sSJQU+3W7t2rXNu4FTs7du3u5KSEuf3+111dbVraWl56MfnNGxWpi0MsP57YKXfetBp2L7/HjhpIxqNKhAIWI8BZIU0+/K+RzZenBb/43nefd/XNz8LDgAwNhEgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMDEsH8fEAAb6X5la2C4eAUEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADAxwXoAANnB5/NZj4AMwysgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYGLYATp58qSWL1+uUCgkn8+ngwcPJtz/6quvyufzJaza2tpkzQsAyBLDDlBvb68qKiq0e/fuIbepra1VZ2dnfO3fv/+RhgQAZJ9h/0bUZcuWadmyZffdxu/3KxgMjngoAED2S8l7QI2NjSouLtbs2bO1ceNGXbt2bchtY7GYotFowgIAZL+kB6i2tlZ/+MMf1NDQoF//+tdqamrSsmXL1NfXN+j2dXV1CgQC8VVWVpbskQAAacjnnHMj/sM+n+rr67VixYoht/nnP/+pmTNn6tixY6qurr7n/lgsplgsFv84Go0SIWAQj/ClOip8Pp/1CEgznucpPz9/yPtTfhr2jBkzVFRUpNbW1kHv9/v9ys/PT1gAgOyX8gBdvnxZ165dU2lpaaqfCgCQQYZ9Ftz169cTXs20t7fr3LlzKiwsVGFhod555x2tXr1awWBQbW1t+ulPf6pZs2Zp6dKlSR0cAJDh3DCdOHHCSbpnrV271t24ccMtWbLETZkyxeXk5Ljp06e79evXu0gk8tCP73neoI/PYo31le6s9w8r/Zbnefc9Zh7pJIRUiEajCgQC1mMAaSfNvlTvwUkIuJv5SQgAAAyGAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJob9+4AAPDqubA3wCggAYIQAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwMQE6wGATOecsx5hSD6fz3oEYEi8AgIAmCBAAAATwwpQXV2dFixYoLy8PBUXF2vFihVqaWlJ2ObWrVsKh8OaPHmynnjiCa1evVpdXV1JHRoAkPmGFaCmpiaFw2GdOnVKR48e1Z07d7RkyRL19vbGt9myZYs++eQTHThwQE1NTbpy5YpWrVqV9MEBABnOPYKrV686Sa6pqck551x3d7fLyclxBw4ciG/z5ZdfOkmuubn5oR7T8zwnicXKmJXOrPcNa2wvz/Pue3w+0ntAnudJkgoLCyVJZ86c0Z07d1RTUxPfZs6cOZo2bZqam5sHfYxYLKZoNJqwAADZb8QB6u/v1+bNm7Vo0SLNnTtXkhSJRJSbm6uCgoKEbUtKShSJRAZ9nLq6OgUCgfgqKysb6UgAgAwy4gCFw2FduHBBH3300SMNsG3bNnmeF1+XLl16pMcDAGSGEf0g6qZNm3T48GGdPHlSU6dOjd8eDAZ1+/ZtdXd3J7wK6urqUjAYHPSx/H6//H7/SMYAAGSwYb0Ccs5p06ZNqq+v1/Hjx1VeXp5w//z585WTk6OGhob4bS0tLero6FBVVVVyJgYAZIVhvQIKh8Pat2+fDh06pLy8vPj7OoFAQBMnTlQgENC6deu0detWFRYWKj8/X2+88Yaqqqr07W9/OyWfAAAgQyXjlM49e/bEt7l586Z7/fXX3ZNPPukmTZrkVq5c6To7Ox/6OTgNm5VpK51Z7xvW2F4POg3b99+DNG1Eo1EFAgHrMYCHlmZfQgm4GCkseZ6n/Pz8Ie/nWnAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwMaLfiApkq3S+sjWQbXgFBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCY4GKkQIbw+XzWIwBJxSsgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAICJCdYDAGORz+ezHgEwxysgAIAJAgQAMDGsANXV1WnBggXKy8tTcXGxVqxYoZaWloRtXnzxRfl8voS1YcOGpA4NAMh8wwpQU1OTwuGwTp06paNHj+rOnTtasmSJent7E7Zbv369Ojs742vXrl1JHRoAkPmGdRLCkSNHEj7eu3eviouLdebMGS1evDh++6RJkxQMBpMzIQAgKz3Se0Ce50mSCgsLE27/8MMPVVRUpLlz52rbtm26cePGkI8Ri8UUjUYTFgBgDHAj1NfX577//e+7RYsWJdz++9//3h05csSdP3/e/fGPf3RPPfWUW7ly5ZCPs3PnTieJxUqLNVqsP08WazSW53n3/Trw/feLYdg2btyov/zlL/r00081derUIbc7fvy4qqur1draqpkzZ95zfywWUywWi38cjUZVVlY2kpGARzbCL4dh4+eAMBZ4nqf8/Pwh7x/RD6Ju2rRJhw8f1smTJ+8bH0mqrKyUpCED5Pf75ff7RzIGACCDDStAzjm98cYbqq+vV2Njo8rLyx/4Z86dOydJKi0tHdGAAIDsNKwAhcNh7du3T4cOHVJeXp4ikYgkKRAIaOLEiWpra9O+ffv0ve99T5MnT9b58+e1ZcsWLV68WPPmzUvJJwAAyFDJeON0z549zjnnOjo63OLFi11hYaHz+/1u1qxZ7q233nrgG1H/n+d55m+cscbuGi3WnyeLNRorZSchpEo0GlUgELAeA2PUaH05cBICxoKUnIQAZCvCAIweLkYKADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACAibQLkHPOegQAQBI86Pt52gWop6fHegQAQBI86Pu5z6XZS47+/n5duXJFeXl58vl8CfdFo1GVlZXp0qVLys/PN5rQHvthAPthAPthAPthQDrsB+ecenp6FAqFNG7c0K9zJoziTA9l3Lhxmjp16n23yc/PH9MH2NfYDwPYDwPYDwPYDwOs90MgEHjgNmn3T3AAgLGBAAEATGRUgPx+v3bu3Cm/3289iin2wwD2wwD2wwD2w4BM2g9pdxICAGBsyKhXQACA7EGAAAAmCBAAwAQBAgCYyJgA7d69W08//bQee+wxVVZW6rPPPrMeadS9/fbb8vl8CWvOnDnWY6XcyZMntXz5coVCIfl8Ph08eDDhfuecduzYodLSUk2cOFE1NTW6ePGizbAp9KD98Oqrr95zfNTW1toMmyJ1dXVasGCB8vLyVFxcrBUrVqilpSVhm1u3bikcDmvy5Ml64okntHr1anV1dRlNnBoPsx9efPHFe46HDRs2GE08uIwI0Mcff6ytW7dq586d+uKLL1RRUaGlS5fq6tWr1qONuueee06dnZ3x9emnn1qPlHK9vb2qqKjQ7t27B71/165dev/99/XBBx/o9OnTevzxx7V06VLdunVrlCdNrQftB0mqra1NOD72798/ihOmXlNTk8LhsE6dOqWjR4/qzp07WrJkiXp7e+PbbNmyRZ988okOHDigpqYmXblyRatWrTKcOvkeZj9I0vr16xOOh127dhlNPASXARYuXOjC4XD8476+PhcKhVxdXZ3hVKNv586drqKiwnoMU5JcfX19/OP+/n4XDAbdu+++G7+tu7vb+f1+t3//foMJR8fd+8E559auXeteeuklk3msXL161UlyTU1NzrmBv/ucnBx34MCB+DZffvmlk+Sam5utxky5u/eDc85997vfdT/+8Y/thnoIaf8K6Pbt2zpz5oxqamrit40bN041NTVqbm42nMzGxYsXFQqFNGPGDL3yyivq6OiwHslUe3u7IpFIwvERCARUWVk5Jo+PxsZGFRcXa/bs2dq4caOuXbtmPVJKeZ4nSSosLJQknTlzRnfu3Ek4HubMmaNp06Zl9fFw93742ocffqiioiLNnTtX27Zt040bNyzGG1LaXYz0bl999ZX6+vpUUlKScHtJSYn+8Y9/GE1lo7KyUnv37tXs2bPV2dmpd955Ry+88IIuXLigvLw86/FMRCIRSRr0+Pj6vrGitrZWq1atUnl5udra2vTzn/9cy5YtU3Nzs8aPH289XtL19/dr8+bNWrRokebOnStp4HjIzc1VQUFBwrbZfDwMth8k6Yc//KGmT5+uUCik8+fP62c/+5laWlr05z//2XDaRGkfIPzPsmXL4v89b948VVZWavr06frTn/6kdevWGU6GdPCDH/wg/t/PP/+85s2bp5kzZ6qxsVHV1dWGk6VGOBzWhQsXxsT7oPcz1H547bXX4v/9/PPPq7S0VNXV1Wpra9PMmTNHe8xBpf0/wRUVFWn8+PH3nMXS1dWlYDBoNFV6KCgo0LPPPqvW1lbrUcx8fQxwfNxrxowZKioqysrjY9OmTTp8+LBOnDiR8OtbgsGgbt++re7u7oTts/V4GGo/DKayslKS0up4SPsA5ebmav78+WpoaIjf1t/fr4aGBlVVVRlOZu/69etqa2tTaWmp9ShmysvLFQwGE46PaDSq06dPj/nj4/Lly7p27VpWHR/OOW3atEn19fU6fvy4ysvLE+6fP3++cnJyEo6HlpYWdXR0ZNXx8KD9MJhz585JUnodD9ZnQTyMjz76yPn9frd3717397//3b322muuoKDARSIR69FG1U9+8hPX2Njo2tvb3V//+ldXU1PjioqK3NWrV61HS6menh539uxZd/bsWSfJvffee+7s2bPu3//+t3POuV/96leuoKDAHTp0yJ0/f9699NJLrry83N28edN48uS6337o6elxb775pmtubnbt7e3u2LFj7pvf/KZ75pln3K1bt6xHT5qNGze6QCDgGhsbXWdnZ3zduHEjvs2GDRvctGnT3PHjx93nn3/uqqqqXFVVleHUyfeg/dDa2up+8YtfuM8//9y1t7e7Q4cOuRkzZrjFixcbT54oIwLknHO//e1v3bRp01xubq5buHChO3XqlPVIo27NmjWutLTU5ebmuqeeesqtWbPGtba2Wo+VcidOnHCS7llr1651zg2cir19+3ZXUlLi/H6/q66udi0tLbZDp8D99sONGzfckiVL3JQpU1xOTo6bPn26W79+fdb9T9pgn78kt2fPnvg2N2/edK+//rp78skn3aRJk9zKlStdZ2en3dAp8KD90NHR4RYvXuwKCwud3+93s2bNcm+99ZbzPM928Lvw6xgAACbS/j0gAEB2IkAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBM/B9Ke37Lfx54IgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bw = np.array(batch[0][0,:]>0, dtype=int)\n",
    "pyplot.imshow(bw.reshape((28, 28)), cmap=\"gray\")\n",
    "print(batch[0][0,:].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d51575-96d3-4c75-ba72-ce7da96123cd",
   "metadata": {},
   "source": [
    "We can do 2D convolutions using the function convolve2d from scipy.signal. Below is an example, where we apply the convolution operator from Slide 29 to the image above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b717ce8-55bb-4dbf-9365-e9aebeee489c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fe249e3a5e0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "kernel = np.array([[1,-1]])\n",
    "res = signal.convolve2d(bw.reshape((28, 28)),kernel, 'same')\n",
    "pyplot.imshow(res.reshape((28, 28)), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0106801-11fb-4619-b026-7afe8a139fa2",
   "metadata": {},
   "source": [
    "## Exercise:\n",
    "\n",
    "* Experiment with different kernels to get an impression of the convolution operator. You may find some inspiration for kernels [here](https://en.wikipedia.org/wiki/Kernel_(image_processing)). If you feel adventuruous, you are most welcome to play around with other images. \n",
    "* Try also taking a closer look at the numerical values being produced to verify your understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2099ce-40d0-46e9-ba5b-6341b845ba12",
   "metadata": {},
   "source": [
    "# Part 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1accb0a1-f013-4594-8fde-f7a272142924",
   "metadata": {},
   "source": [
    "During the last self study session we only made limited use of PyTorch’s functionality for constructing neural networks (basically only using it for calculating gradients). In the self study session below, we will take advantage of much more of its functionality. Specifically, we will start experimenting with convolutional neural networks.\n",
    "\n",
    "The convolution constructs in PyTorch rely on the torch.nn module provided by PyTorch. A short introduction to this module and how to define neural networks in PyTorch can be found at:\n",
    "* https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html#sphx-glr-beginner-blitz-neural-networks-tutorial-py\n",
    "* https://pytorch.org/tutorials/beginner/nn_tutorial.html\n",
    "\n",
    "If you have finished with the previous exercises, you can start preparing for the self study session by going through these tutorials. The former tutorial is part of a general tutorial package to PyTorch, which can be found at:\n",
    "* https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html\n",
    "this also includes a nice introduction to tensors in PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983e1a68-8a1d-4565-96c6-d7eacfcbfe2c",
   "metadata": {},
   "source": [
    "# Self study 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12b8f01c-4d1f-43cf-a6fc-70368a9ea529",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from matplotlib import pyplot\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34055977-f579-4312-84c5-8e82c3ff62c4",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1291c368-5e82-4354-b8f8-badd7adaaf95",
   "metadata": {},
   "source": [
    "As last time we will be working with the MNIST data set: The MNIST database consists of grey scale images of handwritten digits. Each image is of size $28\\times 28$; see figure below for an illustration. The data set is divided into a training set consisting of $60000$ images and a test set with $10000$ images; in both\n",
    "data sets the images are labeled with the correct digits. If interested you can find more information about the MNIST data set at http://yann.lecun.com/exdb/mnist/, including accuracy results for various machine learning methods.\n",
    "\n",
    "![MNIST DATA](MNIST-dataset.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b530d97-218a-4c6b-8c4f-fa48f7c369d2",
   "metadata": {},
   "source": [
    "For this self study, we will be a bit more careful with our data. Specifically, we will divide the data into a training, validation, and test, and use the training and validation set for model learning (in the previous self study we did not have a validation set). \n",
    "\n",
    "The data set is created by setting aside a randomly chosen subset of the data, where the splitting point is found using the help function *split_indicies* below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17433cf6-5c7d-4b33-b69b-d6bc39f6ad7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 48000\n",
      "Number of validation examples: 12000\n"
     ]
    }
   ],
   "source": [
    "def split_indicies(n, val_pct):\n",
    "    # Size of validation set\n",
    "    n_val = int(n*val_pct)\n",
    "    # Random permutation\n",
    "    idxs = np.random.permutation(n)\n",
    "    # Return first indexes for the validation set\n",
    "    return idxs[n_val:], idxs[:n_val]\n",
    "\n",
    "# Load the data\n",
    "train_dataset = datasets.MNIST('../Data', train=True, download=False,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ]))\n",
    "\n",
    "# Get the indicies for the training data and test data (the validation set will consists of 20% of the data)\n",
    "train_idxs, val_idxs = split_indicies(len(train_dataset), 0.2)\n",
    "\n",
    "# Define samplers (used by Dataloader) to the two sets of indicies\n",
    "train_sampler = SubsetRandomSampler(train_idxs)\n",
    "val_sampler = SubsetRandomSampler(val_idxs)\n",
    "\n",
    "# Specify data loaders for our training and test set (same functionality as in the previous self study)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, sampler=train_sampler)\n",
    "val_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, sampler=val_sampler)\n",
    "\n",
    "print(f\"Number of training examples: {len(train_idxs)}\")\n",
    "print(f\"Number of validation examples: {len(val_idxs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c452e873-64e9-4580-83cc-38e50f6ec2d1",
   "metadata": {},
   "source": [
    "The test set is loaded in the usual fashion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d7c567b-1254-456a-b241-1169525b108f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('../Data', train=False, transform=transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])),\n",
    "        batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117de9b0-0350-49b4-9cb2-e78477dd8db6",
   "metadata": {},
   "source": [
    "## Specifying the model\n",
    "\n",
    "When using the _torch.nn_ for specifying our model we subclass the _nn.Module_. The model thus holds all the parameters of the model (see the _init_ function) as well as a specification of the forward step. We don't have to keep track of the backward pass, as PyTorch handles this for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "67d31436-376a-4362-b7d2-983ae45d3bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST_CNN(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Define a convolution operator with 1 input channel, 15 output channels and a kernel size of 5x5\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=15, kernel_size=5)\n",
    "        # Since we are not doing padding (see Lecture 2, Slide 38) the width of the following layer is reduced; for\n",
    "        # each channel the resulting dimension is 24x24. We feed the resulting representation through a linear \n",
    "        # layer, giving 10 values as output - one for each digit.\n",
    "        self.fc = nn.Linear(in_features=15 * 24 * 24, out_features=10)\n",
    "        self.out = None\n",
    "\n",
    "    def forward(self, xb):\n",
    "\n",
    "        # Reshape the input tensor; '-1' indicates that PyTorch will fill-in this \n",
    "        # dimension, whereas the '1' indicates that we only have one color channel. \n",
    "        xb = xb.view(-1, 1, 28, 28)\n",
    "        # Apply convolution and pass the result through a ReLU function\n",
    "        xb = F.relu(self.conv1(xb))\n",
    "        # Reshape the representation\n",
    "        xb = xb.view(-1, 15*24*24)\n",
    "        # Apply the linear layer\n",
    "        xb = self.fc(xb)\n",
    "        # and set the result as the output. Note that we don't take a softmax as this is handled internally in the \n",
    "        # loss function defined below.\n",
    "        self.out = xb\n",
    "\n",
    "        return xb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f322f0c3-0327-481b-9005-5f7fced46174",
   "metadata": {},
   "source": [
    "## Learning and evaluating the model\n",
    "\n",
    "For learning the model, we will use the following function which performs one iteration over the training data. The function also takes an _epoch_ argument, but this is only used for reporting on the learning progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ad619f54-b51e-41a7-bf19-8ed3a02e8dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, loss_fn, epoch):\n",
    "    # Tell PyTorch that this function is part of the training\n",
    "    model.train()\n",
    "\n",
    "    # As optimizer we use stochastic gradient descent as defined by PyTorch. PyTorch also includes a variety \n",
    "    # of other optimizers \n",
    "    learning_rate = 0.01\n",
    "    opt = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Iterate over the training set, one batch at the time, as in the previous self sudy\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # Get the prediction\n",
    "        y_pred = model(data)\n",
    "        \n",
    "        # Remember to zero the gradients so that they don't accumulate\n",
    "        opt.zero_grad()\n",
    "\n",
    "        # Calculate the loss and and the gradients  \n",
    "        loss = loss_fn(y_pred, target)\n",
    "        loss.backward()\n",
    "\n",
    "        # Optimize the parameters by taking one 'step' with the optimizer\n",
    "        opt.step()\n",
    "\n",
    "        # For every 10th batch we output a bit of info\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.sampler),\n",
    "                       100. * batch_idx * len(data) / len(train_loader.sampler), loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a472dd-034c-4f74-8b01-0cc9065fe9be",
   "metadata": {},
   "source": [
    "In the end, we also want to validate our model. To do this we define the function below, which takes a data_loader (either the validation or test set) and reports the model's accuracy and loss on that data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "895bca74-0216-4244-aac3-b71d95298ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, data_loader, loss_fn):\n",
    "    # Tell PyTorch that we are performing evaluation\n",
    "    model.eval()\n",
    "    \n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in data_loader:\n",
    "            output = model(data)\n",
    "            test_loss += loss_fn(output, target).item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(data_loader.batch_sampler)\n",
    "\n",
    "    print('\\nTest/validation set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, \n",
    "        correct, \n",
    "        len(data_loader.sampler),\n",
    "        100. * correct / len(data_loader.sampler)))\n",
    "    \n",
    "    return test_loss, 100. * correct / len(data_loader.sampler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7b0de9-a38b-4997-8fe0-15848b2b5721",
   "metadata": {},
   "source": [
    "## A couple of helper functions\n",
    "\n",
    "Learning a deep neural network can be time consuming, and it might therefore be nice to be able to save and load previously learned models (see also https://pytorch.org/tutorials/beginner/saving_loading_models.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d9bffe10-26ef-477d-bc53-8c81b66fda36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(file_name, model):\n",
    "    torch.save(model, file_name)\n",
    "\n",
    "def load_model(file_name):\n",
    "    model = torch.load(file_name)\n",
    "    model.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f09ef3f-bbfd-4cc8-bbdf-6a364f1a7b7a",
   "metadata": {},
   "source": [
    "## Wrapping things up\n",
    "\n",
    "Finally, we will do the actual learning of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d134d72-2d8f-4cc4-adfe-8e1b9e745ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The number of passes that will be made over the training set\n",
    "num_epochs = 2\n",
    "# torch.nn defines several useful loss-functions, which we will take advantage of here (see Lecture 1, Slide 11, Log-loss).\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "480c2a94-7d9a-4a3c-9786-facb2167ca68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model structure:\n",
      "MNIST_CNN(\n",
      "  (conv1): Conv2d(1, 15, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc): Linear(in_features=8640, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model class\n",
    "model = MNIST_CNN()\n",
    "# and get some information about the structure\n",
    "print('Model structure:')\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29aea5cd-92ba-4762-9ace-7967d6d1d97c",
   "metadata": {},
   "source": [
    "### Iterate over the data set\n",
    "\n",
    "We iterate over the data set for *num_epochs* number of iterations. At each iteration we also calculate the loss/accuracy on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cd9a413c-2e6b-410e-b66d-dbd51f73b082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/48000 (0%)]\tLoss: 0.120050\n",
      "Train Epoch: 0 [640/48000 (1%)]\tLoss: 0.193853\n",
      "Train Epoch: 0 [1280/48000 (3%)]\tLoss: 0.096323\n",
      "Train Epoch: 0 [1920/48000 (4%)]\tLoss: 0.140823\n",
      "Train Epoch: 0 [2560/48000 (5%)]\tLoss: 0.114088\n",
      "Train Epoch: 0 [3200/48000 (7%)]\tLoss: 0.174899\n",
      "Train Epoch: 0 [3840/48000 (8%)]\tLoss: 0.073266\n",
      "Train Epoch: 0 [4480/48000 (9%)]\tLoss: 0.205383\n",
      "Train Epoch: 0 [5120/48000 (11%)]\tLoss: 0.103649\n",
      "Train Epoch: 0 [5760/48000 (12%)]\tLoss: 0.092255\n",
      "Train Epoch: 0 [6400/48000 (13%)]\tLoss: 0.079310\n",
      "Train Epoch: 0 [7040/48000 (15%)]\tLoss: 0.202108\n",
      "Train Epoch: 0 [7680/48000 (16%)]\tLoss: 0.122051\n",
      "Train Epoch: 0 [8320/48000 (17%)]\tLoss: 0.107751\n",
      "Train Epoch: 0 [8960/48000 (19%)]\tLoss: 0.104287\n",
      "Train Epoch: 0 [9600/48000 (20%)]\tLoss: 0.100860\n",
      "Train Epoch: 0 [10240/48000 (21%)]\tLoss: 0.151223\n",
      "Train Epoch: 0 [10880/48000 (23%)]\tLoss: 0.142636\n",
      "Train Epoch: 0 [11520/48000 (24%)]\tLoss: 0.106543\n",
      "Train Epoch: 0 [12160/48000 (25%)]\tLoss: 0.056347\n",
      "Train Epoch: 0 [12800/48000 (27%)]\tLoss: 0.068205\n",
      "Train Epoch: 0 [13440/48000 (28%)]\tLoss: 0.198622\n",
      "Train Epoch: 0 [14080/48000 (29%)]\tLoss: 0.036244\n",
      "Train Epoch: 0 [14720/48000 (31%)]\tLoss: 0.086041\n",
      "Train Epoch: 0 [15360/48000 (32%)]\tLoss: 0.230711\n",
      "Train Epoch: 0 [16000/48000 (33%)]\tLoss: 0.163336\n",
      "Train Epoch: 0 [16640/48000 (35%)]\tLoss: 0.079356\n",
      "Train Epoch: 0 [17280/48000 (36%)]\tLoss: 0.249347\n",
      "Train Epoch: 0 [17920/48000 (37%)]\tLoss: 0.395827\n",
      "Train Epoch: 0 [18560/48000 (39%)]\tLoss: 0.133323\n",
      "Train Epoch: 0 [19200/48000 (40%)]\tLoss: 0.149574\n",
      "Train Epoch: 0 [19840/48000 (41%)]\tLoss: 0.159657\n",
      "Train Epoch: 0 [20480/48000 (43%)]\tLoss: 0.154898\n",
      "Train Epoch: 0 [21120/48000 (44%)]\tLoss: 0.140689\n",
      "Train Epoch: 0 [21760/48000 (45%)]\tLoss: 0.046306\n",
      "Train Epoch: 0 [22400/48000 (47%)]\tLoss: 0.106997\n",
      "Train Epoch: 0 [23040/48000 (48%)]\tLoss: 0.085900\n",
      "Train Epoch: 0 [23680/48000 (49%)]\tLoss: 0.045495\n",
      "Train Epoch: 0 [24320/48000 (51%)]\tLoss: 0.179922\n",
      "Train Epoch: 0 [24960/48000 (52%)]\tLoss: 0.100897\n",
      "Train Epoch: 0 [25600/48000 (53%)]\tLoss: 0.303726\n",
      "Train Epoch: 0 [26240/48000 (55%)]\tLoss: 0.166388\n",
      "Train Epoch: 0 [26880/48000 (56%)]\tLoss: 0.078798\n",
      "Train Epoch: 0 [27520/48000 (57%)]\tLoss: 0.251969\n",
      "Train Epoch: 0 [28160/48000 (59%)]\tLoss: 0.369146\n",
      "Train Epoch: 0 [28800/48000 (60%)]\tLoss: 0.272826\n",
      "Train Epoch: 0 [29440/48000 (61%)]\tLoss: 0.026797\n",
      "Train Epoch: 0 [30080/48000 (63%)]\tLoss: 0.150652\n",
      "Train Epoch: 0 [30720/48000 (64%)]\tLoss: 0.120468\n",
      "Train Epoch: 0 [31360/48000 (65%)]\tLoss: 0.093219\n",
      "Train Epoch: 0 [32000/48000 (67%)]\tLoss: 0.128323\n",
      "Train Epoch: 0 [32640/48000 (68%)]\tLoss: 0.058949\n",
      "Train Epoch: 0 [33280/48000 (69%)]\tLoss: 0.129209\n",
      "Train Epoch: 0 [33920/48000 (71%)]\tLoss: 0.134309\n",
      "Train Epoch: 0 [34560/48000 (72%)]\tLoss: 0.048602\n",
      "Train Epoch: 0 [35200/48000 (73%)]\tLoss: 0.124559\n",
      "Train Epoch: 0 [35840/48000 (75%)]\tLoss: 0.114530\n",
      "Train Epoch: 0 [36480/48000 (76%)]\tLoss: 0.072415\n",
      "Train Epoch: 0 [37120/48000 (77%)]\tLoss: 0.137374\n",
      "Train Epoch: 0 [37760/48000 (79%)]\tLoss: 0.123866\n",
      "Train Epoch: 0 [38400/48000 (80%)]\tLoss: 0.051243\n",
      "Train Epoch: 0 [39040/48000 (81%)]\tLoss: 0.060851\n",
      "Train Epoch: 0 [39680/48000 (83%)]\tLoss: 0.072328\n",
      "Train Epoch: 0 [40320/48000 (84%)]\tLoss: 0.083385\n",
      "Train Epoch: 0 [40960/48000 (85%)]\tLoss: 0.148863\n",
      "Train Epoch: 0 [41600/48000 (87%)]\tLoss: 0.114402\n",
      "Train Epoch: 0 [42240/48000 (88%)]\tLoss: 0.078607\n",
      "Train Epoch: 0 [42880/48000 (89%)]\tLoss: 0.130558\n",
      "Train Epoch: 0 [43520/48000 (91%)]\tLoss: 0.049165\n",
      "Train Epoch: 0 [44160/48000 (92%)]\tLoss: 0.174300\n",
      "Train Epoch: 0 [44800/48000 (93%)]\tLoss: 0.117826\n",
      "Train Epoch: 0 [45440/48000 (95%)]\tLoss: 0.096863\n",
      "Train Epoch: 0 [46080/48000 (96%)]\tLoss: 0.148715\n",
      "Train Epoch: 0 [46720/48000 (97%)]\tLoss: 0.060311\n",
      "Train Epoch: 0 [47360/48000 (99%)]\tLoss: 0.057870\n",
      "\n",
      "Test/validation set: Average loss: 0.1052, Accuracy: 11656/12000 (97%)\n",
      "\n",
      "Train Epoch: 1 [0/48000 (0%)]\tLoss: 0.185463\n",
      "Train Epoch: 1 [640/48000 (1%)]\tLoss: 0.034270\n",
      "Train Epoch: 1 [1280/48000 (3%)]\tLoss: 0.310134\n",
      "Train Epoch: 1 [1920/48000 (4%)]\tLoss: 0.031793\n",
      "Train Epoch: 1 [2560/48000 (5%)]\tLoss: 0.067408\n",
      "Train Epoch: 1 [3200/48000 (7%)]\tLoss: 0.094771\n",
      "Train Epoch: 1 [3840/48000 (8%)]\tLoss: 0.075730\n",
      "Train Epoch: 1 [4480/48000 (9%)]\tLoss: 0.045613\n",
      "Train Epoch: 1 [5120/48000 (11%)]\tLoss: 0.064294\n",
      "Train Epoch: 1 [5760/48000 (12%)]\tLoss: 0.055077\n",
      "Train Epoch: 1 [6400/48000 (13%)]\tLoss: 0.189394\n",
      "Train Epoch: 1 [7040/48000 (15%)]\tLoss: 0.107296\n",
      "Train Epoch: 1 [7680/48000 (16%)]\tLoss: 0.055496\n",
      "Train Epoch: 1 [8320/48000 (17%)]\tLoss: 0.112429\n",
      "Train Epoch: 1 [8960/48000 (19%)]\tLoss: 0.172155\n",
      "Train Epoch: 1 [9600/48000 (20%)]\tLoss: 0.108066\n",
      "Train Epoch: 1 [10240/48000 (21%)]\tLoss: 0.061162\n",
      "Train Epoch: 1 [10880/48000 (23%)]\tLoss: 0.119132\n",
      "Train Epoch: 1 [11520/48000 (24%)]\tLoss: 0.079440\n",
      "Train Epoch: 1 [12160/48000 (25%)]\tLoss: 0.051113\n",
      "Train Epoch: 1 [12800/48000 (27%)]\tLoss: 0.086327\n",
      "Train Epoch: 1 [13440/48000 (28%)]\tLoss: 0.020445\n",
      "Train Epoch: 1 [14080/48000 (29%)]\tLoss: 0.066726\n",
      "Train Epoch: 1 [14720/48000 (31%)]\tLoss: 0.093379\n",
      "Train Epoch: 1 [15360/48000 (32%)]\tLoss: 0.027072\n",
      "Train Epoch: 1 [16000/48000 (33%)]\tLoss: 0.141934\n",
      "Train Epoch: 1 [16640/48000 (35%)]\tLoss: 0.030547\n",
      "Train Epoch: 1 [17280/48000 (36%)]\tLoss: 0.095644\n",
      "Train Epoch: 1 [17920/48000 (37%)]\tLoss: 0.107774\n",
      "Train Epoch: 1 [18560/48000 (39%)]\tLoss: 0.073472\n",
      "Train Epoch: 1 [19200/48000 (40%)]\tLoss: 0.058596\n",
      "Train Epoch: 1 [19840/48000 (41%)]\tLoss: 0.037735\n",
      "Train Epoch: 1 [20480/48000 (43%)]\tLoss: 0.152970\n",
      "Train Epoch: 1 [21120/48000 (44%)]\tLoss: 0.235132\n",
      "Train Epoch: 1 [21760/48000 (45%)]\tLoss: 0.232605\n",
      "Train Epoch: 1 [22400/48000 (47%)]\tLoss: 0.122279\n",
      "Train Epoch: 1 [23040/48000 (48%)]\tLoss: 0.156870\n",
      "Train Epoch: 1 [23680/48000 (49%)]\tLoss: 0.053798\n",
      "Train Epoch: 1 [24320/48000 (51%)]\tLoss: 0.097384\n",
      "Train Epoch: 1 [24960/48000 (52%)]\tLoss: 0.095007\n",
      "Train Epoch: 1 [25600/48000 (53%)]\tLoss: 0.214252\n",
      "Train Epoch: 1 [26240/48000 (55%)]\tLoss: 0.106322\n",
      "Train Epoch: 1 [26880/48000 (56%)]\tLoss: 0.065685\n",
      "Train Epoch: 1 [27520/48000 (57%)]\tLoss: 0.160314\n",
      "Train Epoch: 1 [28160/48000 (59%)]\tLoss: 0.125248\n",
      "Train Epoch: 1 [28800/48000 (60%)]\tLoss: 0.121001\n",
      "Train Epoch: 1 [29440/48000 (61%)]\tLoss: 0.061856\n",
      "Train Epoch: 1 [30080/48000 (63%)]\tLoss: 0.197435\n",
      "Train Epoch: 1 [30720/48000 (64%)]\tLoss: 0.079169\n",
      "Train Epoch: 1 [31360/48000 (65%)]\tLoss: 0.075713\n",
      "Train Epoch: 1 [32000/48000 (67%)]\tLoss: 0.132543\n",
      "Train Epoch: 1 [32640/48000 (68%)]\tLoss: 0.100280\n",
      "Train Epoch: 1 [33280/48000 (69%)]\tLoss: 0.020572\n",
      "Train Epoch: 1 [33920/48000 (71%)]\tLoss: 0.045524\n",
      "Train Epoch: 1 [34560/48000 (72%)]\tLoss: 0.084283\n",
      "Train Epoch: 1 [35200/48000 (73%)]\tLoss: 0.087840\n",
      "Train Epoch: 1 [35840/48000 (75%)]\tLoss: 0.143301\n",
      "Train Epoch: 1 [36480/48000 (76%)]\tLoss: 0.075470\n",
      "Train Epoch: 1 [37120/48000 (77%)]\tLoss: 0.021612\n",
      "Train Epoch: 1 [37760/48000 (79%)]\tLoss: 0.246957\n",
      "Train Epoch: 1 [38400/48000 (80%)]\tLoss: 0.043850\n",
      "Train Epoch: 1 [39040/48000 (81%)]\tLoss: 0.095908\n",
      "Train Epoch: 1 [39680/48000 (83%)]\tLoss: 0.175675\n",
      "Train Epoch: 1 [40320/48000 (84%)]\tLoss: 0.108728\n",
      "Train Epoch: 1 [40960/48000 (85%)]\tLoss: 0.029260\n",
      "Train Epoch: 1 [41600/48000 (87%)]\tLoss: 0.041203\n",
      "Train Epoch: 1 [42240/48000 (88%)]\tLoss: 0.198601\n",
      "Train Epoch: 1 [42880/48000 (89%)]\tLoss: 0.126470\n",
      "Train Epoch: 1 [43520/48000 (91%)]\tLoss: 0.049412\n",
      "Train Epoch: 1 [44160/48000 (92%)]\tLoss: 0.017670\n",
      "Train Epoch: 1 [44800/48000 (93%)]\tLoss: 0.028636\n",
      "Train Epoch: 1 [45440/48000 (95%)]\tLoss: 0.117347\n",
      "Train Epoch: 1 [46080/48000 (96%)]\tLoss: 0.110661\n",
      "Train Epoch: 1 [46720/48000 (97%)]\tLoss: 0.020683\n",
      "Train Epoch: 1 [47360/48000 (99%)]\tLoss: 0.270647\n",
      "\n",
      "Test/validation set: Average loss: 0.0895, Accuracy: 11699/12000 (97%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(num_epochs):\n",
    "    train(model, train_loader, loss_fn, i)\n",
    "    # Evaluate the model on the test set\n",
    "    test_model(model, val_loader, loss_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2dc8c7-acd8-4be7-9d9d-364ab8b51848",
   "metadata": {},
   "source": [
    "After learning we evaluate the model on the _test set_ and save the resulting structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc59f415-4c16-41f8-8ec9-a8c06dd5c6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "test_model(model, test_loader, loss_fn)\n",
    "# Save the model\n",
    "save_model('conv.pt', model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c745b31-6631-4cbe-8bd8-860c35d0d565",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Self Study Exercises\n",
    "\n",
    "1. Familiarize yourself with the code above and consult the PyTorch documentation when needed.\n",
    "2. Experiment with different NN architectures (also varying the convolutional parameters: size, stride, padding, etc) and observe the effect wrt. the loss/accuracy on the training and validation dataset (training, validation). Note that when adding new layers (including dropout [Lecture 2, Slide 13], pooling, etc.) you need to make sure that the dimensionality of the layers match up. **IMPORTANT:** ignore the test set at this stage (i.e., comment out the relevant lines above) so that the results for the test set do not influence your model choice.\n",
    "3. In the model above we use a simple gradient descent learning scheme. Try other types of optimizers (see https://pytorch.org/docs/stable/optim.html) and analyze the effect.\n",
    "4. If you feel adventurous, try investigating some of the other datasets that come prepacakged with PyTorch (see https://pytorch.org/vision/0.8/datasets.html). For instnce, for FashionMNIST you only need to change the dataloader from datasets.MNIST to datasets.FashionMNIST."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd99d25",
   "metadata": {},
   "source": [
    "### Experiment with different NN architectures (also varying the convolutional parameters: size, stride, padding, etc) and observe the effect wrt. the loss/accuracy on the training and validation dataset (training, validation). Note that when adding new layers (including dropout [Lecture 2, Slide 13], pooling, etc.) you need to make sure that the dimensionality of the layers match up."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884ab873",
   "metadata": {},
   "source": [
    "Three different experiments:\n",
    "1. Different kernel sizes: 3x3, 7x7, 5x5 (baseline)\n",
    "2. Different stride values: 1 (baseline), 2, 3\n",
    "3. With max pooling layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "614a704f-b12b-487e-b0e2-8eb24c8d9081",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST_CNN_3x3(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Define a convolution operator with 1 input channel, 15 output channels and a kernel size of 3x3\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=15, kernel_size=3)\n",
    "        # Since we are not doing padding, the width of the following layer is reduced; for\n",
    "        # each channel the resulting dimension is 26x26. We feed the resulting representation through a linear \n",
    "        # layer, giving 10 values as output - one for each digit.\n",
    "        self.fc = nn.Linear(in_features=15 * 26 * 26, out_features=10)\n",
    "        self.out = None\n",
    "\n",
    "    def forward(self, xb):\n",
    "\n",
    "        # Reshape the input tensor; '-1' indicates that PyTorch will fill-in this \n",
    "        # dimension, whereas the '1' indicates that we only have one color channel. \n",
    "        xb = xb.view(-1, 1, 28, 28)\n",
    "        # Apply convolution and pass the result through a ReLU function\n",
    "        xb = F.relu(self.conv1(xb))\n",
    "        # Reshape the representation\n",
    "        xb = xb.view(-1, 15*26*26)\n",
    "        # Apply the linear layer\n",
    "        xb = self.fc(xb)\n",
    "        # and set the result as the output. Note that we don't take a softmax as this is handled internally in the \n",
    "        # loss function defined below.\n",
    "        self.out = xb\n",
    "\n",
    "        return xb\n",
    "\n",
    "\n",
    "class MNIST_CNN_7x7(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Define a convolution operator with 1 input channel, 15 output channels and a kernel size of 7x7\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=15, kernel_size=7)\n",
    "        # Since we are not doing padding, the width of the following layer is reduced; for\n",
    "        # each channel the resulting dimension is 22x22. We feed the resulting representation through a linear \n",
    "        # layer, giving 10 values as output - one for each digit.\n",
    "        self.fc = nn.Linear(in_features=15 * 22 * 22, out_features=10)\n",
    "        self.out = None\n",
    "\n",
    "    def forward(self, xb):\n",
    "\n",
    "        # Reshape the input tensor; '-1' indicates that PyTorch will fill-in this \n",
    "        # dimension, whereas the '1' indicates that we only have one color channel. \n",
    "        xb = xb.view(-1, 1, 28, 28)\n",
    "        # Apply convolution and pass the result through a ReLU function\n",
    "        xb = F.relu(self.conv1(xb))\n",
    "        # Reshape the representation\n",
    "        xb = xb.view(-1, 15*22*22)\n",
    "        # Apply the linear layer\n",
    "        xb = self.fc(xb)\n",
    "        # and set the result as the output. Note that we don't take a softmax as this is handled internally in the \n",
    "        # loss function defined below.\n",
    "        self.out = xb\n",
    "\n",
    "        return xb\n",
    "    \n",
    "    \n",
    "class MNIST_CNN_MAXPOOL(nn.Module):  \n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Define a convolution operator with 1 input channel, 15 output channels and a kernel size of 5x5\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=15, kernel_size=5)\n",
    "        # Add a maxpooling layer with kernel size 2x2\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2)\n",
    "        # Since we are not doing padding, the width of the following layer is reduced; for\n",
    "        # each channel the resulting dimension is 12x12. We feed the resulting representation through a linear \n",
    "        # layer, giving 10 values as output - one for each digit.\n",
    "        self.fc = nn.Linear(in_features=15 * 12 * 12, out_features=10)\n",
    "        self.out = None\n",
    "\n",
    "    def forward(self, xb):\n",
    "\n",
    "        # Reshape the input tensor; '-1' indicates that PyTorch will fill-in this \n",
    "        # dimension, whereas the '1' indicates that we only have one color channel. \n",
    "        xb = xb.view(-1, 1, 28, 28)\n",
    "        # Apply convolution and pass the result through a ReLU function\n",
    "        xb = F.relu(self.conv1(xb))\n",
    "        # Apply maxpooling\n",
    "        xb = self.pool(xb)\n",
    "        # Reshape the representation\n",
    "        xb = xb.view(-1, 15*12*12)\n",
    "        # Apply the linear layer\n",
    "        xb = self.fc(xb)\n",
    "        # and set the result as the output. Note that we don't take a softmax as this is handled internally in the \n",
    "        # loss function defined below.\n",
    "        self.out = xb\n",
    "\n",
    "        return xb\n",
    "    \n",
    "    \n",
    "class MNIST_CNN_STRIDE_2(nn.Module):  \n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Define a convolution operator with 1 input channel, 15 output channels and a kernel size of 5x5\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=15, kernel_size=5, stride=2)\n",
    "        self.fc = nn.Linear(in_features=15 * 12 * 12, out_features=10)\n",
    "        self.out = None\n",
    "\n",
    "    def forward(self, xb):\n",
    "\n",
    "        # Reshape the input tensor; '-1' indicates that PyTorch will fill-in this \n",
    "        # dimension, whereas the '1' indicates that we only have one color channel. \n",
    "        xb = xb.view(-1, 1, 28, 28)\n",
    "        # Apply convolution and pass the result through a ReLU function\n",
    "        xb = F.relu(self.conv1(xb))\n",
    "        # Reshape the representation\n",
    "        xb = xb.view(-1, 15*12*12)\n",
    "        # Apply the linear layer\n",
    "        xb = self.fc(xb)\n",
    "        # and set the result as the output. Note that we don't take a softmax as this is handled internally in the \n",
    "        # loss function defined below.\n",
    "        self.out = xb\n",
    "\n",
    "        return xb\n",
    "    \n",
    "    \n",
    "class MNIST_CNN_STRIDE_3(nn.Module):  \n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Define a convolution operator with 1 input channel, 15 output channels and a kernel size of 5x5\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=15, kernel_size=5, stride=3)\n",
    "        self.fc = nn.Linear(in_features=15 * 8 * 8, out_features=10)\n",
    "        self.out = None\n",
    "\n",
    "    def forward(self, xb):\n",
    "\n",
    "        # Reshape the input tensor; '-1' indicates that PyTorch will fill-in this \n",
    "        # dimension, whereas the '1' indicates that we only have one color channel. \n",
    "        xb = xb.view(-1, 1, 28, 28)\n",
    "        # Apply convolution and pass the result through a ReLU function\n",
    "        xb = F.relu(self.conv1(xb))\n",
    "        # Reshape the representation\n",
    "        xb = xb.view(-1, 15*8*8)\n",
    "        # Apply the linear layer\n",
    "        xb = self.fc(xb)\n",
    "        # and set the result as the output. Note that we don't take a softmax as this is handled internally in the \n",
    "        # loss function defined below.\n",
    "        self.out = xb\n",
    "\n",
    "        return xb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4d69e5db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0:\n",
      "MNIST_CNN(\n",
      "  (conv1): Conv2d(1, 15, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc): Linear(in_features=8640, out_features=10, bias=True)\n",
      ")\n",
      "Train Epoch: 0 [0/48000 (0%)]\tLoss: 2.333903\n",
      "Train Epoch: 0 [6400/48000 (13%)]\tLoss: 0.498634\n",
      "Train Epoch: 0 [12800/48000 (27%)]\tLoss: 0.292980\n",
      "Train Epoch: 0 [19200/48000 (40%)]\tLoss: 0.373997\n",
      "Train Epoch: 0 [25600/48000 (53%)]\tLoss: 0.203954\n",
      "Train Epoch: 0 [32000/48000 (67%)]\tLoss: 0.230935\n",
      "Train Epoch: 0 [38400/48000 (80%)]\tLoss: 0.144755\n",
      "Train Epoch: 0 [44800/48000 (93%)]\tLoss: 0.181634\n",
      "\n",
      "Test/validation set: Average loss: 0.1762, Accuracy: 45580/48000 (95%)\n",
      "\n",
      "\n",
      "Test/validation set: Average loss: 0.1783, Accuracy: 11392/12000 (95%)\n",
      "\n",
      "Train Epoch: 1 [0/48000 (0%)]\tLoss: 0.066364\n",
      "Train Epoch: 1 [6400/48000 (13%)]\tLoss: 0.337470\n",
      "Train Epoch: 1 [12800/48000 (27%)]\tLoss: 0.066322\n",
      "Train Epoch: 1 [19200/48000 (40%)]\tLoss: 0.180113\n",
      "Train Epoch: 1 [25600/48000 (53%)]\tLoss: 0.199789\n",
      "Train Epoch: 1 [32000/48000 (67%)]\tLoss: 0.191820\n",
      "Train Epoch: 1 [38400/48000 (80%)]\tLoss: 0.134336\n",
      "Train Epoch: 1 [44800/48000 (93%)]\tLoss: 0.069745\n",
      "\n",
      "Test/validation set: Average loss: 0.1170, Accuracy: 46463/48000 (97%)\n",
      "\n",
      "\n",
      "Test/validation set: Average loss: 0.1218, Accuracy: 11583/12000 (97%)\n",
      "\n",
      "Model 1:\n",
      "MNIST_CNN_3x3(\n",
      "  (conv1): Conv2d(1, 15, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc): Linear(in_features=10140, out_features=10, bias=True)\n",
      ")\n",
      "Train Epoch: 0 [0/48000 (0%)]\tLoss: 2.307859\n",
      "Train Epoch: 0 [6400/48000 (13%)]\tLoss: 0.617895\n",
      "Train Epoch: 0 [12800/48000 (27%)]\tLoss: 0.679553\n",
      "Train Epoch: 0 [19200/48000 (40%)]\tLoss: 0.257499\n",
      "Train Epoch: 0 [25600/48000 (53%)]\tLoss: 0.357171\n",
      "Train Epoch: 0 [32000/48000 (67%)]\tLoss: 0.150249\n",
      "Train Epoch: 0 [38400/48000 (80%)]\tLoss: 0.348639\n",
      "Train Epoch: 0 [44800/48000 (93%)]\tLoss: 0.281949\n",
      "\n",
      "Test/validation set: Average loss: 0.2666, Accuracy: 44426/48000 (93%)\n",
      "\n",
      "\n",
      "Test/validation set: Average loss: 0.2686, Accuracy: 11091/12000 (92%)\n",
      "\n",
      "Train Epoch: 1 [0/48000 (0%)]\tLoss: 0.374941\n",
      "Train Epoch: 1 [6400/48000 (13%)]\tLoss: 0.210077\n",
      "Train Epoch: 1 [12800/48000 (27%)]\tLoss: 0.232879\n",
      "Train Epoch: 1 [19200/48000 (40%)]\tLoss: 0.422684\n",
      "Train Epoch: 1 [25600/48000 (53%)]\tLoss: 0.144089\n",
      "Train Epoch: 1 [32000/48000 (67%)]\tLoss: 0.193663\n",
      "Train Epoch: 1 [38400/48000 (80%)]\tLoss: 0.156475\n",
      "Train Epoch: 1 [44800/48000 (93%)]\tLoss: 0.167924\n",
      "\n",
      "Test/validation set: Average loss: 0.2040, Accuracy: 45253/48000 (94%)\n",
      "\n",
      "\n",
      "Test/validation set: Average loss: 0.2108, Accuracy: 11288/12000 (94%)\n",
      "\n",
      "Model 2:\n",
      "MNIST_CNN_7x7(\n",
      "  (conv1): Conv2d(1, 15, kernel_size=(7, 7), stride=(1, 1))\n",
      "  (fc): Linear(in_features=7260, out_features=10, bias=True)\n",
      ")\n",
      "Train Epoch: 0 [0/48000 (0%)]\tLoss: 2.298489\n",
      "Train Epoch: 0 [6400/48000 (13%)]\tLoss: 0.323071\n",
      "Train Epoch: 0 [12800/48000 (27%)]\tLoss: 0.500297\n",
      "Train Epoch: 0 [19200/48000 (40%)]\tLoss: 0.313527\n",
      "Train Epoch: 0 [25600/48000 (53%)]\tLoss: 0.288869\n",
      "Train Epoch: 0 [32000/48000 (67%)]\tLoss: 0.170000\n",
      "Train Epoch: 0 [38400/48000 (80%)]\tLoss: 0.125304\n",
      "Train Epoch: 0 [44800/48000 (93%)]\tLoss: 0.119895\n",
      "\n",
      "Test/validation set: Average loss: 0.1711, Accuracy: 45657/48000 (95%)\n",
      "\n",
      "\n",
      "Test/validation set: Average loss: 0.1726, Accuracy: 11398/12000 (95%)\n",
      "\n",
      "Train Epoch: 1 [0/48000 (0%)]\tLoss: 0.119966\n",
      "Train Epoch: 1 [6400/48000 (13%)]\tLoss: 0.067059\n",
      "Train Epoch: 1 [12800/48000 (27%)]\tLoss: 0.146291\n",
      "Train Epoch: 1 [19200/48000 (40%)]\tLoss: 0.160928\n",
      "Train Epoch: 1 [25600/48000 (53%)]\tLoss: 0.282013\n",
      "Train Epoch: 1 [32000/48000 (67%)]\tLoss: 0.090323\n",
      "Train Epoch: 1 [38400/48000 (80%)]\tLoss: 0.209219\n",
      "Train Epoch: 1 [44800/48000 (93%)]\tLoss: 0.059914\n",
      "\n",
      "Test/validation set: Average loss: 0.1247, Accuracy: 46260/48000 (96%)\n",
      "\n",
      "\n",
      "Test/validation set: Average loss: 0.1276, Accuracy: 11555/12000 (96%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_accuracy = []\n",
    "\n",
    "baseline_model = MNIST_CNN()\n",
    "model_3x3 = MNIST_CNN_3x3()\n",
    "model_7x7 = MNIST_CNN_7x7()\n",
    "\n",
    "\n",
    "for i, model in enumerate([baseline_model, model_3x3, model_7x7]):\n",
    "    print(f\"Model {i}:\")\n",
    "    print(model)\n",
    "    model_results = []\n",
    "    for j in range(num_epochs):\n",
    "        train(model, train_loader, loss_fn, j)\n",
    "        train_loss, train_accuracy = test_model(model, train_loader, loss_fn)\n",
    "        val_loss, val_accuracy = test_model(model, val_loader, loss_fn)\n",
    "        model_accuracy.append((train_accuracy, val_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "231e53c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(94.95833333333333, 94.93333333333334),\n",
       " (96.79791666666667, 96.525),\n",
       " (92.55416666666666, 92.425),\n",
       " (94.27708333333334, 94.06666666666666),\n",
       " (95.11875, 94.98333333333333),\n",
       " (96.375, 96.29166666666667)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "25e6878f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model structure:\n",
      "MNIST_CNN_MAXPOOL(\n",
      "  (conv1): Conv2d(1, 15, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc): Linear(in_features=2160, out_features=10, bias=True)\n",
      ")\n",
      "Train Epoch: 0 [0/48000 (0%)]\tLoss: 2.414359\n",
      "Train Epoch: 0 [6400/48000 (13%)]\tLoss: 0.510592\n",
      "Train Epoch: 0 [12800/48000 (27%)]\tLoss: 0.628543\n",
      "Train Epoch: 0 [19200/48000 (40%)]\tLoss: 0.296346\n",
      "Train Epoch: 0 [25600/48000 (53%)]\tLoss: 0.175421\n",
      "Train Epoch: 0 [32000/48000 (67%)]\tLoss: 0.242708\n",
      "Train Epoch: 0 [38400/48000 (80%)]\tLoss: 0.475020\n",
      "Train Epoch: 0 [44800/48000 (93%)]\tLoss: 0.224372\n",
      "\n",
      "Test/validation set: Average loss: 0.2139, Accuracy: 45129/48000 (94%)\n",
      "\n",
      "\n",
      "Test/validation set: Average loss: 0.2105, Accuracy: 11266/12000 (94%)\n",
      "\n",
      "Train Epoch: 1 [0/48000 (0%)]\tLoss: 0.133787\n",
      "Train Epoch: 1 [6400/48000 (13%)]\tLoss: 0.160485\n",
      "Train Epoch: 1 [12800/48000 (27%)]\tLoss: 0.267972\n",
      "Train Epoch: 1 [19200/48000 (40%)]\tLoss: 0.277899\n",
      "Train Epoch: 1 [25600/48000 (53%)]\tLoss: 0.148780\n",
      "Train Epoch: 1 [32000/48000 (67%)]\tLoss: 0.349231\n",
      "Train Epoch: 1 [38400/48000 (80%)]\tLoss: 0.111762\n",
      "Train Epoch: 1 [44800/48000 (93%)]\tLoss: 0.132052\n",
      "\n",
      "Test/validation set: Average loss: 0.1413, Accuracy: 46148/48000 (96%)\n",
      "\n",
      "\n",
      "Test/validation set: Average loss: 0.1405, Accuracy: 11533/12000 (96%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "maxpool_model = MNIST_CNN_MAXPOOL()\n",
    "print('Model structure:')\n",
    "print(maxpool_model)\n",
    "results = []\n",
    "\n",
    "for i in range(num_epochs):\n",
    "    train(maxpool_model, train_loader, loss_fn, i)\n",
    "    # Evaluate the model on the test set\n",
    "    results.append((test_model(maxpool_model, train_loader, loss_fn)))\n",
    "    results.append((test_model(maxpool_model, val_loader, loss_fn)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0e361cd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.21392822141448656, 94.01875),\n",
       " (0.2105454359837669, 93.88333333333334),\n",
       " (0.14132943468292555, 96.14166666666667),\n",
       " (0.14053769433434973, 96.10833333333333)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bf1bbae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0:\n",
      "MNIST_CNN(\n",
      "  (conv1): Conv2d(1, 15, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc): Linear(in_features=8640, out_features=10, bias=True)\n",
      ")\n",
      "Train Epoch: 0 [0/48000 (0%)]\tLoss: 0.056725\n",
      "Train Epoch: 0 [6400/48000 (13%)]\tLoss: 0.023092\n",
      "Train Epoch: 0 [12800/48000 (27%)]\tLoss: 0.033804\n",
      "Train Epoch: 0 [19200/48000 (40%)]\tLoss: 0.045623\n",
      "Train Epoch: 0 [25600/48000 (53%)]\tLoss: 0.018926\n",
      "Train Epoch: 0 [32000/48000 (67%)]\tLoss: 0.058311\n",
      "Train Epoch: 0 [38400/48000 (80%)]\tLoss: 0.111579\n",
      "Train Epoch: 0 [44800/48000 (93%)]\tLoss: 0.012651\n",
      "\n",
      "Test/validation set: Average loss: 0.0691, Accuracy: 47102/48000 (98%)\n",
      "\n",
      "\n",
      "Test/validation set: Average loss: 0.0850, Accuracy: 11698/12000 (97%)\n",
      "\n",
      "Train Epoch: 1 [0/48000 (0%)]\tLoss: 0.152011\n",
      "Train Epoch: 1 [6400/48000 (13%)]\tLoss: 0.133597\n",
      "Train Epoch: 1 [12800/48000 (27%)]\tLoss: 0.057294\n",
      "Train Epoch: 1 [19200/48000 (40%)]\tLoss: 0.040005\n",
      "Train Epoch: 1 [25600/48000 (53%)]\tLoss: 0.065459\n",
      "Train Epoch: 1 [32000/48000 (67%)]\tLoss: 0.006521\n",
      "Train Epoch: 1 [38400/48000 (80%)]\tLoss: 0.092230\n",
      "Train Epoch: 1 [44800/48000 (93%)]\tLoss: 0.027963\n",
      "\n",
      "Test/validation set: Average loss: 0.0603, Accuracy: 47210/48000 (98%)\n",
      "\n",
      "\n",
      "Test/validation set: Average loss: 0.0787, Accuracy: 11727/12000 (98%)\n",
      "\n",
      "Model 1:\n",
      "MNIST_CNN_STRIDE_2(\n",
      "  (conv1): Conv2d(1, 15, kernel_size=(5, 5), stride=(2, 2))\n",
      "  (fc): Linear(in_features=2160, out_features=10, bias=True)\n",
      ")\n",
      "Train Epoch: 0 [0/48000 (0%)]\tLoss: 2.282716\n",
      "Train Epoch: 0 [6400/48000 (13%)]\tLoss: 0.533422\n",
      "Train Epoch: 0 [12800/48000 (27%)]\tLoss: 0.414695\n",
      "Train Epoch: 0 [19200/48000 (40%)]\tLoss: 0.390383\n",
      "Train Epoch: 0 [25600/48000 (53%)]\tLoss: 0.460898\n",
      "Train Epoch: 0 [32000/48000 (67%)]\tLoss: 0.187411\n",
      "Train Epoch: 0 [38400/48000 (80%)]\tLoss: 0.451920\n",
      "Train Epoch: 0 [44800/48000 (93%)]\tLoss: 0.232474\n",
      "\n",
      "Test/validation set: Average loss: 0.2728, Accuracy: 44251/48000 (92%)\n",
      "\n",
      "\n",
      "Test/validation set: Average loss: 0.2729, Accuracy: 11051/12000 (92%)\n",
      "\n",
      "Train Epoch: 1 [0/48000 (0%)]\tLoss: 0.136107\n",
      "Train Epoch: 1 [6400/48000 (13%)]\tLoss: 0.223154\n",
      "Train Epoch: 1 [12800/48000 (27%)]\tLoss: 0.466250\n",
      "Train Epoch: 1 [19200/48000 (40%)]\tLoss: 0.180317\n",
      "Train Epoch: 1 [25600/48000 (53%)]\tLoss: 0.099718\n",
      "Train Epoch: 1 [32000/48000 (67%)]\tLoss: 0.265324\n",
      "Train Epoch: 1 [38400/48000 (80%)]\tLoss: 0.102606\n",
      "Train Epoch: 1 [44800/48000 (93%)]\tLoss: 0.332301\n",
      "\n",
      "Test/validation set: Average loss: 0.1933, Accuracy: 45455/48000 (95%)\n",
      "\n",
      "\n",
      "Test/validation set: Average loss: 0.1952, Accuracy: 11333/12000 (94%)\n",
      "\n",
      "Model 2:\n",
      "MNIST_CNN_STRIDE_3(\n",
      "  (conv1): Conv2d(1, 15, kernel_size=(5, 5), stride=(3, 3))\n",
      "  (fc): Linear(in_features=960, out_features=10, bias=True)\n",
      ")\n",
      "Train Epoch: 0 [0/48000 (0%)]\tLoss: 2.322564\n",
      "Train Epoch: 0 [6400/48000 (13%)]\tLoss: 0.757412\n",
      "Train Epoch: 0 [12800/48000 (27%)]\tLoss: 0.480963\n",
      "Train Epoch: 0 [19200/48000 (40%)]\tLoss: 0.692303\n",
      "Train Epoch: 0 [25600/48000 (53%)]\tLoss: 0.398256\n",
      "Train Epoch: 0 [32000/48000 (67%)]\tLoss: 0.294543\n",
      "Train Epoch: 0 [38400/48000 (80%)]\tLoss: 0.352047\n",
      "Train Epoch: 0 [44800/48000 (93%)]\tLoss: 0.285907\n",
      "\n",
      "Test/validation set: Average loss: 0.3366, Accuracy: 43358/48000 (90%)\n",
      "\n",
      "\n",
      "Test/validation set: Average loss: 0.3394, Accuracy: 10804/12000 (90%)\n",
      "\n",
      "Train Epoch: 1 [0/48000 (0%)]\tLoss: 0.240423\n",
      "Train Epoch: 1 [6400/48000 (13%)]\tLoss: 0.218695\n",
      "Train Epoch: 1 [12800/48000 (27%)]\tLoss: 0.243273\n",
      "Train Epoch: 1 [19200/48000 (40%)]\tLoss: 0.307482\n",
      "Train Epoch: 1 [25600/48000 (53%)]\tLoss: 0.236852\n",
      "Train Epoch: 1 [32000/48000 (67%)]\tLoss: 0.325771\n",
      "Train Epoch: 1 [38400/48000 (80%)]\tLoss: 0.269846\n",
      "Train Epoch: 1 [44800/48000 (93%)]\tLoss: 0.291716\n",
      "\n",
      "Test/validation set: Average loss: 0.2814, Accuracy: 44108/48000 (92%)\n",
      "\n",
      "\n",
      "Test/validation set: Average loss: 0.2830, Accuracy: 11004/12000 (92%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_accuracy_stride = []\n",
    "model_stride_2 = MNIST_CNN_STRIDE_2()\n",
    "model_stride_3 = MNIST_CNN_STRIDE_3()\n",
    "\n",
    "for i, model in enumerate([baseline_model, model_stride_2, model_stride_3]):\n",
    "    print(f\"Model {i}:\")\n",
    "    print(model)\n",
    "    for j in range(num_epochs):\n",
    "        train(model, train_loader, loss_fn, j)\n",
    "        train_loss, train_accuracy = test_model(model, train_loader, loss_fn)\n",
    "        val_loss, val_accuracy = test_model(model, val_loader, loss_fn)\n",
    "        model_accuracy_stride.append((train_accuracy, val_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4544f76f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(98.12916666666666, 97.48333333333333),\n",
       " (98.35416666666667, 97.725),\n",
       " (92.18958333333333, 92.09166666666667),\n",
       " (94.69791666666667, 94.44166666666666),\n",
       " (90.32916666666667, 90.03333333333333),\n",
       " (91.89166666666667, 91.7)]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_accuracy_stride"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7e5059",
   "metadata": {},
   "source": [
    "**Architecture**:\n",
    "1. **Input layer**: \n",
    "    * Expects a single-channel (grayscale) image of size 28x28 pixels.\n",
    "2. **Convolutional layer**:\n",
    "    * A convolutional layer with a $m \\times n$ kernel (here $m=n$).\n",
    "    * 1 input channel (grey-scale image) and 15 output channels.\n",
    "    * Applies ReLU activation function after convolution.\n",
    "3. **Output Layer**:\n",
    "    * A fully connected (linear) layer that takes the flattened feature vector as input.\n",
    "    * Outputs 10 values corresponding to the 10 digit classes (0 through 9).\n",
    "    * Typically applies a softmax activation to convert output neurons to probabilities (applied in loss function in this case)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832328fd",
   "metadata": {},
   "source": [
    "**Experiment 1 - different kernel sizes in convolutional layer:**\n",
    "* $\\text{CNN}_{3 \\times 3 \\text{kernel}}$:\n",
    "    * $\\text{Accuracy}_{\\text{train}} = 94.28$\n",
    "    * $\\text{Accuracy}_{\\text{test}} = 94.06$\n",
    "* $\\text{CNN}_{5 \\times 5 \\text{kernel}}$:\n",
    "    * $\\text{Accuracy}_{\\text{train}} = 96.79$\n",
    "    * $\\text{Accuracy}_{\\text{test}} = 96.53$\n",
    "* $\\text{CNN}_{7 \\times 7 \\text{kernel}}$:\n",
    "    * $\\text{Accuracy}_{\\text{train}} = 96.375$\n",
    "    * $\\text{Accuracy}_{\\text{test}} = 96.29$\n",
    "\n",
    "Observations:\n",
    "* $3 \\times 3$ kernel perform the worst while the other kernel sizes have almost equivalent performance. Thus, larger kernel capture seems to derive more relevant features.\n",
    "* Larger kernels have a larger receptive field and capture more 'global' information within the input image.\n",
    "* Larger kernels also introduce more parameters which comes with a additional computational cost and memory usage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9581a20",
   "metadata": {},
   "source": [
    "**Experiment 2 - different stride sizes:**\n",
    "* $\\text{CNN}_{1 \\text{stride}}$:\n",
    "    * $\\text{Accuracy}_{\\text{train}} = 96.79$\n",
    "    * $\\text{Accuracy}_{\\text{test}} = 96.53$\n",
    "* $\\text{CNN}_{2 \\text{stride}}$:\n",
    "    * $\\text{Accuracy}_{\\text{train}} = 94.7$\n",
    "    * $\\text{Accuracy}_{\\text{test}} = 94.44$\n",
    "* $\\text{CNN}_{3 \\text{stride}}$:\n",
    "    * $\\text{Accuracy}_{\\text{train}} = 91.89$\n",
    "    * $\\text{Accuracy}_{\\text{test}} = 91.7$\n",
    "\n",
    "Observations:\n",
    "* Increasing stride decreases accuracy. \n",
    "* Natural since more spatial information is lost and mnist images are low-resolution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ef24f2",
   "metadata": {},
   "source": [
    "**Experiment 3 - Addition of 2x2 max pooling layer** \n",
    "\n",
    "Results:\n",
    "* $\\text{Accuracy}_{\\text{train}} = 96.14$\n",
    "* $\\text{Accuracy}_{\\text{test}} = 96.10$\n",
    "\n",
    "Observations:\n",
    "* Minor reduction in performance compared to baseline model.\n",
    "* Translation invariance potentially not as significant in mnist dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b3aa08",
   "metadata": {},
   "source": [
    "**Other hyperparameters and architectures not tested**\n",
    "* Dropout layer\n",
    "* Number of channels in convolutional layer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter_kernel",
   "language": "python",
   "name": "jupyter_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
