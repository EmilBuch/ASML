{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ab51a06-4ee6-4819-a45f-e19e1c6989b8",
   "metadata": {},
   "source": [
    "# Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7fab82-b5fd-4ec3-ad00-58cd60cdf435",
   "metadata": {},
   "source": [
    "Revisit the Jupyter notebook from the previous self study session; there we implemented a simple neural network using functionality from numpy and PyTorch. \n",
    "\n",
    "### Exercise: \n",
    "* Modify the code by introducing regularization (L1 and/or L2). \n",
    "* How does the regularization affect the neural network results and the weights being learned? Try also experimenting with different weights for the regularization term."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7768dd-a1e5-4fd2-a9e3-c19801e0ffaa",
   "metadata": {},
   "source": [
    "# Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478031b5-5f9c-4070-80be-ca6c629bb8f6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Exercise\n",
    "\n",
    "In this exercise, the intention is to get a bit more handson experience with the convolution operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "32ce2dd4-be5d-4d73-8a01-0f6b58ec72b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pathlib import Path\n",
    "import requests\n",
    "from matplotlib import pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "import gzip\n",
    "import os\n",
    "from scipy import signal\n",
    "from torchvision import datasets, transforms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b714a973-0b07-480c-a551-7119b6cda60a",
   "metadata": {},
   "source": [
    "The MNIST database consists of grey scale images of handwritten digits. Each image is of size $28\\times 28$; see figure below for an illustration. The data set is divided into a training set, validation set, and test set consisting of $50000$, $10000$, and $10000$ images, respectively; in all data sets the images are labeled with the correct digits. If interested, you can find more information about the MNIST data set at http://yann.lecun.com/exdb/mnist/, including accuracy results for various machine learning methods.\n",
    "\n",
    "![MNIST DATA](MNIST-dataset.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb8faa3-2a97-40b4-be4a-733fa8f1cfc8",
   "metadata": {},
   "source": [
    "First we download the dataset and unpackage it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6881a6cf-ba30-4018-8335-57af1172b77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path(\"data\")\n",
    "PATH = DATA_PATH / \"mnist\"\n",
    "\n",
    "PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "URL = \"https://github.com/pytorch/tutorials/raw/master/_static/\"\n",
    "# URL = \"http://deeplearning.net/data/mnist/\"\n",
    "FILENAME = \"mnist.pkl.gz\"\n",
    "\n",
    "if not (PATH / FILENAME).exists():\n",
    "        content = requests.get(URL + FILENAME).content\n",
    "        (PATH / FILENAME).open(\"wb\").write(content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9127ce8f-06b9-4423-be70-6ae788fc4ccc",
   "metadata": {},
   "source": [
    "We then extract the data and store it numpy arrays: x_train, y_train, x_valid, y_valid, x_test, y_test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2d8d4b37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/vugs/Codes/AAU_Projects/E23/ASML/data/mnist/mnist.pkl.gz\n"
     ]
    }
   ],
   "source": [
    "ROOT_PATH = os.path.normpath(\n",
    "    os.path.join(os.getcwd(), \"E23\", \"ASML\", os.pardir, os.pardir)\n",
    ")\n",
    "FILE_PATH = os.path.join(ROOT_PATH, \"data\", \"mnist\", FILENAME)\n",
    "print(FILE_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "88d16b84-0ed8-4daa-8568-2105c3eed26b",
   "metadata": {},
   "outputs": [
    {
     "ename": "BadGzipFile",
     "evalue": "Not a gzipped file (b'\\n\\n')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBadGzipFile\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/home/vugs/Codes/AAU_Projects/E23/ASML/ASML-2023-2_neural_networks.ipynb Cell 11\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/vugs/Codes/AAU_Projects/E23/ASML/ASML-2023-2_neural_networks.ipynb#X12sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mwith\u001b[39;00m gzip\u001b[39m.\u001b[39mopen(FILE_PATH, \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/vugs/Codes/AAU_Projects/E23/ASML/ASML-2023-2_neural_networks.ipynb#X12sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m         ((x_train, y_train), (x_valid, y_valid), (x_test, y_test)) \u001b[39m=\u001b[39m pickle\u001b[39m.\u001b[39;49mload(f, encoding\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mlatin-1\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m/usr/lib/python3.10/gzip.py:321\u001b[0m, in \u001b[0;36mGzipFile.peek\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    319\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39merrno\u001b[39;00m\n\u001b[1;32m    320\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mOSError\u001b[39;00m(errno\u001b[39m.\u001b[39mEBADF, \u001b[39m\"\u001b[39m\u001b[39mpeek() on write-only GzipFile object\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 321\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_buffer\u001b[39m.\u001b[39;49mpeek(n)\n",
      "File \u001b[0;32m/usr/lib/python3.10/_compression.py:68\u001b[0m, in \u001b[0;36mDecompressReader.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mreadinto\u001b[39m(\u001b[39mself\u001b[39m, b):\n\u001b[1;32m     67\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mmemoryview\u001b[39m(b) \u001b[39mas\u001b[39;00m view, view\u001b[39m.\u001b[39mcast(\u001b[39m\"\u001b[39m\u001b[39mB\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m byte_view:\n\u001b[0;32m---> 68\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m(byte_view))\n\u001b[1;32m     69\u001b[0m         byte_view[:\u001b[39mlen\u001b[39m(data)] \u001b[39m=\u001b[39m data\n\u001b[1;32m     70\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mlen\u001b[39m(data)\n",
      "File \u001b[0;32m/usr/lib/python3.10/gzip.py:488\u001b[0m, in \u001b[0;36m_GzipReader.read\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    484\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_new_member:\n\u001b[1;32m    485\u001b[0m     \u001b[39m# If the _new_member flag is set, we have to\u001b[39;00m\n\u001b[1;32m    486\u001b[0m     \u001b[39m# jump to the next member, if there is one.\u001b[39;00m\n\u001b[1;32m    487\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_read()\n\u001b[0;32m--> 488\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_read_gzip_header():\n\u001b[1;32m    489\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_size \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pos\n\u001b[1;32m    490\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[0;32m/usr/lib/python3.10/gzip.py:436\u001b[0m, in \u001b[0;36m_GzipReader._read_gzip_header\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    433\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    435\u001b[0m \u001b[39mif\u001b[39;00m magic \u001b[39m!=\u001b[39m \u001b[39mb\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\037\u001b[39;00m\u001b[39m\\213\u001b[39;00m\u001b[39m'\u001b[39m:\n\u001b[0;32m--> 436\u001b[0m     \u001b[39mraise\u001b[39;00m BadGzipFile(\u001b[39m'\u001b[39m\u001b[39mNot a gzipped file (\u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m magic)\n\u001b[1;32m    438\u001b[0m (method, flag,\n\u001b[1;32m    439\u001b[0m  \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_last_mtime) \u001b[39m=\u001b[39m struct\u001b[39m.\u001b[39munpack(\u001b[39m\"\u001b[39m\u001b[39m<BBIxx\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_read_exact(\u001b[39m8\u001b[39m))\n\u001b[1;32m    440\u001b[0m \u001b[39mif\u001b[39;00m method \u001b[39m!=\u001b[39m \u001b[39m8\u001b[39m:\n",
      "\u001b[0;31mBadGzipFile\u001b[0m: Not a gzipped file (b'\\n\\n')"
     ]
    }
   ],
   "source": [
    "with gzip.open(FILE_PATH, \"rb\") as f:\n",
    "        ((x_train, y_train), (x_valid, y_valid), (x_test, y_test)) = pickle.load(f, encoding=\"latin-1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8254a9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('../data', train=True, download=True,\n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize((0.1307,), (0.3081,))\n",
    "                       ])),\n",
    "        shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])),\n",
    "        shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "36cf8df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = enumerate(train_loader)\n",
    "_, (x_train, y_train) = next(training_data)\n",
    "\n",
    "test_data = enumerate(test_loader)\n",
    "_, (x_test, y_test) = next(test_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a504f9e9-ed17-441b-bdf6-713525bea0a5",
   "metadata": {},
   "source": [
    "Check the shape of the x_valid holding the validation data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f50004-4593-4a37-a9c4-3592573b7dd8",
   "metadata": {},
   "source": [
    "The images are stored in rows of length $784$, hence to display the images we need to reshape them to $28\\times 28$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ce92d745-28ca-4657-8ea8-52e55f4c1771",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 28, 28])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pyplot.imshow(x_train[0].reshape((28, 28)), cmap=\"gray\")\n",
    "print(x_train.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3832adbe-4018-443b-bcaf-d81906b9d0ac",
   "metadata": {},
   "source": [
    "We can also convert the image into black and white"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3b6edfd4-3541-4864-b385-4b46ae9c4c70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 28, 28])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bw = np.array(x_train[0]>0, dtype=int)\n",
    "pyplot.imshow(bw.reshape((28, 28)), cmap=\"gray\")\n",
    "print(x_train.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d51575-96d3-4c75-ba72-ce7da96123cd",
   "metadata": {},
   "source": [
    "We can do 2D convolutions using the function convolve2d from scipy.signal. Below is an example, where we apply the convolution operator from Slide 29 to the image above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "0b717ce8-55bb-4dbf-9365-e9aebeee489c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f7ff018a0e0>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "kernel = np.array([[0,-1, 0], [-1, 8, -1], [0,-1, 0]])\n",
    "res = signal.convolve2d(bw.reshape((28, 28)),kernel, 'same')\n",
    "pyplot.imshow(res.reshape((28, 28)), cmap=\"gray\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0106801-11fb-4619-b026-7afe8a139fa2",
   "metadata": {},
   "source": [
    "## Exercise:\n",
    "\n",
    "* Experiment with different kernels to get an impression of the convolution operator. You may find some inspiration for kernels [here](https://en.wikipedia.org/wiki/Kernel_(image_processing)). If you feel adventuruous, you are most welcome to play around with other images. \n",
    "* Try also taking a closer look at the numerical values being produced to verify your understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2099ce-40d0-46e9-ba5b-6341b845ba12",
   "metadata": {},
   "source": [
    "# Part 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1accb0a1-f013-4594-8fde-f7a272142924",
   "metadata": {},
   "source": [
    "During the last self study session we only made limited use of PyTorch’s functionality for constructing neural networks (basically only using it for calculating gradients). In the self study session below, we will take advantage of much more of its functionality. Specifically, we will start experimenting with convolutional neural networks.\n",
    "\n",
    "The convolution constructs in PyTorch rely on the torch.nn module provided by PyTorch. A short introduction to this module and how to define neural networks in PyTorch can be found at:\n",
    "* https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html#sphx-glr-beginner-blitz-neural-networks-tutorial-py\n",
    "* https://pytorch.org/tutorials/beginner/nn_tutorial.html\n",
    "\n",
    "If you have finished with the previous exercises, you can start preparing for the self study session by going through these tutorials. The former tutorial is part of a general tutorial package to PyTorch, which can be found at:\n",
    "* https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html\n",
    "this also includes a nice introduction to tensors in PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983e1a68-8a1d-4565-96c6-d7eacfcbfe2c",
   "metadata": {},
   "source": [
    "# Self study 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "12b8f01c-4d1f-43cf-a6fc-70368a9ea529",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from matplotlib import pyplot\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34055977-f579-4312-84c5-8e82c3ff62c4",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1291c368-5e82-4354-b8f8-badd7adaaf95",
   "metadata": {},
   "source": [
    "As last time we will be working with the MNIST data set: The MNIST database consists of grey scale images of handwritten digits. Each image is of size $28\\times 28$; see figure below for an illustration. The data set is divided into a training set consisting of $60000$ images and a test set with $10000$ images; in both\n",
    "data sets the images are labeled with the correct digits. If interested you can find more information about the MNIST data set at http://yann.lecun.com/exdb/mnist/, including accuracy results for various machine learning methods.\n",
    "\n",
    "![MNIST DATA](MNIST-dataset.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b530d97-218a-4c6b-8c4f-fa48f7c369d2",
   "metadata": {},
   "source": [
    "For this self study, we will be a bit more careful with our data. Specifically, we will divide the data into a training, validation, and test, and use the training and validation set for model learning (in the previous self study we did not have a validation set). \n",
    "\n",
    "The data set is created by setting aside a randomly chosen subset of the data, where the splitting point is found using the help function *split_indicies* below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "17433cf6-5c7d-4b33-b69b-d6bc39f6ad7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 48000\n",
      "Number of validation examples: 12000\n"
     ]
    }
   ],
   "source": [
    "def split_indicies(n, val_pct):\n",
    "    # Size of validation set\n",
    "    n_val = int(n*val_pct)\n",
    "    # Random permutation\n",
    "    idxs = np.random.permutation(n)\n",
    "    # Return first indexes for the validation set\n",
    "    return idxs[n_val:], idxs[:n_val]\n",
    "\n",
    "# Load the data\n",
    "train_dataset = datasets.MNIST('../data', train=True, download=False,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ]))\n",
    "\n",
    "# Get the indicies for the training data and test data (the validation set will consists of 20% of the data)\n",
    "train_idxs, val_idxs = split_indicies(len(train_dataset), 0.2)\n",
    "\n",
    "# Define samplers (used by Dataloader) to the two sets of indicies\n",
    "train_sampler = SubsetRandomSampler(train_idxs)\n",
    "val_sampler = SubsetRandomSampler(val_idxs)\n",
    "\n",
    "# Specify data loaders for our training and test set (same functionality as in the previous self study)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, sampler=train_sampler)\n",
    "val_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, sampler=val_sampler)\n",
    "\n",
    "print(f\"Number of training examples: {len(train_idxs)}\")\n",
    "print(f\"Number of validation examples: {len(val_idxs)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c452e873-64e9-4580-83cc-38e50f6ec2d1",
   "metadata": {},
   "source": [
    "The test set is loaded in the usual fashion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5d7c567b-1254-456a-b241-1169525b108f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])),\n",
    "        batch_size=64, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117de9b0-0350-49b4-9cb2-e78477dd8db6",
   "metadata": {},
   "source": [
    "## Specifying the model\n",
    "\n",
    "When using the _torch.nn_ for specifying our model we subclass the _nn.Module_. The model thus holds all the parameters of the model (see the _init_ function) as well as a specification of the forward step. We don't have to keep track of the backward pass, as PyTorch handles this for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "67d31436-376a-4362-b7d2-983ae45d3bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST_CNN(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Define a convolution operator with 1 input channel, 15 output channels and a kernel size of 5x5\n",
    "        self.conv1 = nn.Conv2d(1, 15, 5)\n",
    "        # Since we are not doing padding (see Lecture 2, Slide 38) the width of the following layer is reduced; for\n",
    "        # each channel the resulting dimension is 24x24. We feed the resulting representation through a linear\n",
    "        # layer, giving 10 values as output - one for each digit.\n",
    "        self.fc = nn.Linear(15 * 12 * 12, 10)\n",
    "        self.out = None\n",
    "\n",
    "    def forward(self, xb):\n",
    "\n",
    "        # Reshape the input tensor; '-1' indicates that PyTorch will fill-in this\n",
    "        # dimension, whereas the '1' indicates that we only have one color channel.\n",
    "        xb = xb.view(-1, 1, 28, 28)\n",
    "        # Apply convolution and pass the result through a ReLU function\n",
    "        xb = F.max_pool2d(F.relu(self.conv1(xb)), 2)\n",
    "        # Reshape the representation\n",
    "        xb = xb.view(-1, 15*12*12)\n",
    "        # Apply the linear layer\n",
    "        xb = self.fc(xb)\n",
    "        # and set the result as the output. Note that we don't take a softmax as this is handled internally in the\n",
    "        # loss function defined below.\n",
    "        self.out = xb\n",
    "\n",
    "        return xb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f322f0c3-0327-481b-9005-5f7fced46174",
   "metadata": {},
   "source": [
    "## Learning and evaluating the model\n",
    "\n",
    "For learning the model, we will use the following function which performs one iteration over the training data. The function also takes an _epoch_ argument, but this is only used for reporting on the learning progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "ad619f54-b51e-41a7-bf19-8ed3a02e8dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, loss_fn, epoch):\n",
    "    # Tell PyTorch that this function is part of the training\n",
    "    model.train()\n",
    "\n",
    "    # As optimizer we use stochastic gradient descent as defined by PyTorch. PyTorch also includes a variety\n",
    "    # of other optimizers\n",
    "    learning_rate = 0.01\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Iterate over the training set, one batch at the time, as in the previous self sudy\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # Get the prediction\n",
    "        y_pred = model(data)\n",
    "\n",
    "        # Remember to zero the gradients so that they don't accumulate\n",
    "        opt.zero_grad()\n",
    "\n",
    "        # Calculate the loss and and the gradients\n",
    "        loss = loss_fn(y_pred, target)\n",
    "        loss.backward()\n",
    "\n",
    "        # Optimize the parameters by taking one 'step' with the optimizer\n",
    "        opt.step()\n",
    "\n",
    "        # For every 10th batch we output a bit of info\n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.sampler),\n",
    "                       100. * batch_idx * len(data) / len(train_loader.sampler), loss.item()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a472dd-034c-4f74-8b01-0cc9065fe9be",
   "metadata": {},
   "source": [
    "In the end, we also want to validate our model. To do this we define the function below, which takes a data_loader (either the validation or test set) and reports the model's accuracy and loss on that data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "895bca74-0216-4244-aac3-b71d95298ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, data_loader, loss_fn):\n",
    "    # Tell PyTorch that we are performing evaluation\n",
    "    model.eval()\n",
    "\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in data_loader:\n",
    "            output = model(data)\n",
    "            test_loss += loss_fn(output, target).item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(data_loader.dataset)\n",
    "\n",
    "    print('\\nTest/validation set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(data_loader.sampler),\n",
    "        100. * correct / len(data_loader.sampler)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7b0de9-a38b-4997-8fe0-15848b2b5721",
   "metadata": {},
   "source": [
    "## A couple of helper functions\n",
    "\n",
    "Learning a deep neural network can be time consuming, and it might therefore be nice to be able to save and load previously learned models (see also https://pytorch.org/tutorials/beginner/saving_loading_models.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "d9bffe10-26ef-477d-bc53-8c81b66fda36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(file_name, model):\n",
    "    torch.save(model, file_name)\n",
    "\n",
    "def load_model(file_name):\n",
    "    model = torch.load(file_name)\n",
    "    model.eval()\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f09ef3f-bbfd-4cc8-bbdf-6a364f1a7b7a",
   "metadata": {},
   "source": [
    "## Wrapping things up\n",
    "\n",
    "Finally, we will do the actual learning of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "7d134d72-2d8f-4cc4-adfe-8e1b9e745ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The number of passes that will be made over the training set\n",
    "num_epochs = 2\n",
    "# torch.nn defines several useful loss-functions, which we will take advantage of here (see Lecture 1, Slide 11, Log-loss).\n",
    "loss_fn = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "480c2a94-7d9a-4a3c-9786-facb2167ca68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model structure:\n",
      "MNIST_CNN(\n",
      "  (conv1): Conv2d(1, 15, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc): Linear(in_features=2160, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model class\n",
    "model = MNIST_CNN()\n",
    "# and get some information about the structure\n",
    "print('Model structure:')\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29aea5cd-92ba-4762-9ace-7967d6d1d97c",
   "metadata": {},
   "source": [
    "### Iterate over the data set\n",
    "\n",
    "We iterate over the data set for *num_epochs* number of iterations. At each iteration we also calculate the loss/accuracy on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "cd9a413c-2e6b-410e-b66d-dbd51f73b082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/48000 (0%)]\tLoss: 2.421450\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [640/48000 (1%)]\tLoss: 1.045835\n",
      "Train Epoch: 0 [1280/48000 (3%)]\tLoss: 0.323610\n",
      "Train Epoch: 0 [1920/48000 (4%)]\tLoss: 0.493709\n",
      "Train Epoch: 0 [2560/48000 (5%)]\tLoss: 0.304164\n",
      "Train Epoch: 0 [3200/48000 (7%)]\tLoss: 0.118532\n",
      "Train Epoch: 0 [3840/48000 (8%)]\tLoss: 0.253364\n",
      "Train Epoch: 0 [4480/48000 (9%)]\tLoss: 0.146564\n",
      "Train Epoch: 0 [5120/48000 (11%)]\tLoss: 0.150264\n",
      "Train Epoch: 0 [5760/48000 (12%)]\tLoss: 0.110048\n",
      "Train Epoch: 0 [6400/48000 (13%)]\tLoss: 0.121779\n",
      "Train Epoch: 0 [7040/48000 (15%)]\tLoss: 0.218325\n",
      "Train Epoch: 0 [7680/48000 (16%)]\tLoss: 0.108332\n",
      "Train Epoch: 0 [8320/48000 (17%)]\tLoss: 0.155337\n",
      "Train Epoch: 0 [8960/48000 (19%)]\tLoss: 0.125181\n",
      "Train Epoch: 0 [9600/48000 (20%)]\tLoss: 0.214799\n",
      "Train Epoch: 0 [10240/48000 (21%)]\tLoss: 0.197805\n",
      "Train Epoch: 0 [10880/48000 (23%)]\tLoss: 0.069804\n",
      "Train Epoch: 0 [11520/48000 (24%)]\tLoss: 0.070703\n",
      "Train Epoch: 0 [12160/48000 (25%)]\tLoss: 0.123635\n",
      "Train Epoch: 0 [12800/48000 (27%)]\tLoss: 0.281086\n",
      "Train Epoch: 0 [13440/48000 (28%)]\tLoss: 0.094598\n",
      "Train Epoch: 0 [14080/48000 (29%)]\tLoss: 0.116678\n",
      "Train Epoch: 0 [14720/48000 (31%)]\tLoss: 0.227173\n",
      "Train Epoch: 0 [15360/48000 (32%)]\tLoss: 0.055533\n",
      "Train Epoch: 0 [16000/48000 (33%)]\tLoss: 0.040356\n",
      "Train Epoch: 0 [16640/48000 (35%)]\tLoss: 0.329591\n",
      "Train Epoch: 0 [17280/48000 (36%)]\tLoss: 0.090907\n",
      "Train Epoch: 0 [17920/48000 (37%)]\tLoss: 0.046209\n",
      "Train Epoch: 0 [18560/48000 (39%)]\tLoss: 0.245732\n",
      "Train Epoch: 0 [19200/48000 (40%)]\tLoss: 0.136121\n",
      "Train Epoch: 0 [19840/48000 (41%)]\tLoss: 0.083767\n",
      "Train Epoch: 0 [20480/48000 (43%)]\tLoss: 0.063192\n",
      "Train Epoch: 0 [21120/48000 (44%)]\tLoss: 0.041048\n",
      "Train Epoch: 0 [21760/48000 (45%)]\tLoss: 0.048850\n",
      "Train Epoch: 0 [22400/48000 (47%)]\tLoss: 0.190892\n",
      "Train Epoch: 0 [23040/48000 (48%)]\tLoss: 0.057911\n",
      "Train Epoch: 0 [23680/48000 (49%)]\tLoss: 0.141027\n",
      "Train Epoch: 0 [24320/48000 (51%)]\tLoss: 0.177129\n",
      "Train Epoch: 0 [24960/48000 (52%)]\tLoss: 0.077242\n",
      "Train Epoch: 0 [25600/48000 (53%)]\tLoss: 0.174970\n",
      "Train Epoch: 0 [26240/48000 (55%)]\tLoss: 0.154531\n",
      "Train Epoch: 0 [26880/48000 (56%)]\tLoss: 0.042449\n",
      "Train Epoch: 0 [27520/48000 (57%)]\tLoss: 0.081036\n",
      "Train Epoch: 0 [28160/48000 (59%)]\tLoss: 0.011802\n",
      "Train Epoch: 0 [28800/48000 (60%)]\tLoss: 0.047424\n",
      "Train Epoch: 0 [29440/48000 (61%)]\tLoss: 0.118287\n",
      "Train Epoch: 0 [30080/48000 (63%)]\tLoss: 0.122572\n",
      "Train Epoch: 0 [30720/48000 (64%)]\tLoss: 0.103751\n",
      "Train Epoch: 0 [31360/48000 (65%)]\tLoss: 0.027123\n",
      "Train Epoch: 0 [32000/48000 (67%)]\tLoss: 0.113152\n",
      "Train Epoch: 0 [32640/48000 (68%)]\tLoss: 0.118868\n",
      "Train Epoch: 0 [33280/48000 (69%)]\tLoss: 0.039620\n",
      "Train Epoch: 0 [33920/48000 (71%)]\tLoss: 0.203208\n",
      "Train Epoch: 0 [34560/48000 (72%)]\tLoss: 0.034196\n",
      "Train Epoch: 0 [35200/48000 (73%)]\tLoss: 0.159234\n",
      "Train Epoch: 0 [35840/48000 (75%)]\tLoss: 0.141208\n",
      "Train Epoch: 0 [36480/48000 (76%)]\tLoss: 0.260188\n",
      "Train Epoch: 0 [37120/48000 (77%)]\tLoss: 0.065167\n",
      "Train Epoch: 0 [37760/48000 (79%)]\tLoss: 0.147841\n",
      "Train Epoch: 0 [38400/48000 (80%)]\tLoss: 0.007643\n",
      "Train Epoch: 0 [39040/48000 (81%)]\tLoss: 0.142088\n",
      "Train Epoch: 0 [39680/48000 (83%)]\tLoss: 0.044473\n",
      "Train Epoch: 0 [40320/48000 (84%)]\tLoss: 0.076514\n",
      "Train Epoch: 0 [40960/48000 (85%)]\tLoss: 0.126551\n",
      "Train Epoch: 0 [41600/48000 (87%)]\tLoss: 0.043343\n",
      "Train Epoch: 0 [42240/48000 (88%)]\tLoss: 0.112365\n",
      "Train Epoch: 0 [42880/48000 (89%)]\tLoss: 0.008529\n",
      "Train Epoch: 0 [43520/48000 (91%)]\tLoss: 0.055945\n",
      "Train Epoch: 0 [44160/48000 (92%)]\tLoss: 0.105612\n",
      "Train Epoch: 0 [44800/48000 (93%)]\tLoss: 0.045253\n",
      "Train Epoch: 0 [45440/48000 (95%)]\tLoss: 0.061146\n",
      "Train Epoch: 0 [46080/48000 (96%)]\tLoss: 0.123637\n",
      "Train Epoch: 0 [46720/48000 (97%)]\tLoss: 0.325521\n",
      "Train Epoch: 0 [47360/48000 (99%)]\tLoss: 0.112460\n",
      "\n",
      "Test/validation set: Average loss: 0.0003, Accuracy: 11679/12000 (97%)\n",
      "\n",
      "Train Epoch: 1 [0/48000 (0%)]\tLoss: 0.048586\n",
      "Train Epoch: 1 [640/48000 (1%)]\tLoss: 0.161835\n",
      "Train Epoch: 1 [1280/48000 (3%)]\tLoss: 0.116271\n",
      "Train Epoch: 1 [1920/48000 (4%)]\tLoss: 0.027094\n",
      "Train Epoch: 1 [2560/48000 (5%)]\tLoss: 0.034735\n",
      "Train Epoch: 1 [3200/48000 (7%)]\tLoss: 0.061110\n",
      "Train Epoch: 1 [3840/48000 (8%)]\tLoss: 0.138460\n",
      "Train Epoch: 1 [4480/48000 (9%)]\tLoss: 0.081755\n",
      "Train Epoch: 1 [5120/48000 (11%)]\tLoss: 0.080890\n",
      "Train Epoch: 1 [5760/48000 (12%)]\tLoss: 0.433631\n",
      "Train Epoch: 1 [6400/48000 (13%)]\tLoss: 0.125860\n",
      "Train Epoch: 1 [7040/48000 (15%)]\tLoss: 0.252547\n",
      "Train Epoch: 1 [7680/48000 (16%)]\tLoss: 0.109586\n",
      "Train Epoch: 1 [8320/48000 (17%)]\tLoss: 0.047429\n",
      "Train Epoch: 1 [8960/48000 (19%)]\tLoss: 0.013201\n",
      "Train Epoch: 1 [9600/48000 (20%)]\tLoss: 0.149221\n",
      "Train Epoch: 1 [10240/48000 (21%)]\tLoss: 0.021925\n",
      "Train Epoch: 1 [10880/48000 (23%)]\tLoss: 0.196862\n",
      "Train Epoch: 1 [11520/48000 (24%)]\tLoss: 0.159650\n",
      "Train Epoch: 1 [12160/48000 (25%)]\tLoss: 0.055573\n",
      "Train Epoch: 1 [12800/48000 (27%)]\tLoss: 0.016954\n",
      "Train Epoch: 1 [13440/48000 (28%)]\tLoss: 0.178393\n",
      "Train Epoch: 1 [14080/48000 (29%)]\tLoss: 0.141410\n",
      "Train Epoch: 1 [14720/48000 (31%)]\tLoss: 0.143453\n",
      "Train Epoch: 1 [15360/48000 (32%)]\tLoss: 0.306987\n",
      "Train Epoch: 1 [16000/48000 (33%)]\tLoss: 0.038116\n",
      "Train Epoch: 1 [16640/48000 (35%)]\tLoss: 0.252468\n",
      "Train Epoch: 1 [17280/48000 (36%)]\tLoss: 0.153683\n",
      "Train Epoch: 1 [17920/48000 (37%)]\tLoss: 0.094212\n",
      "Train Epoch: 1 [18560/48000 (39%)]\tLoss: 0.119937\n",
      "Train Epoch: 1 [19200/48000 (40%)]\tLoss: 0.031614\n",
      "Train Epoch: 1 [19840/48000 (41%)]\tLoss: 0.164293\n",
      "Train Epoch: 1 [20480/48000 (43%)]\tLoss: 0.045751\n",
      "Train Epoch: 1 [21120/48000 (44%)]\tLoss: 0.058921\n",
      "Train Epoch: 1 [21760/48000 (45%)]\tLoss: 0.149119\n",
      "Train Epoch: 1 [22400/48000 (47%)]\tLoss: 0.136957\n",
      "Train Epoch: 1 [23040/48000 (48%)]\tLoss: 0.246458\n",
      "Train Epoch: 1 [23680/48000 (49%)]\tLoss: 0.064741\n",
      "Train Epoch: 1 [24320/48000 (51%)]\tLoss: 0.037824\n",
      "Train Epoch: 1 [24960/48000 (52%)]\tLoss: 0.084680\n",
      "Train Epoch: 1 [25600/48000 (53%)]\tLoss: 0.100093\n",
      "Train Epoch: 1 [26240/48000 (55%)]\tLoss: 0.043266\n",
      "Train Epoch: 1 [26880/48000 (56%)]\tLoss: 0.147339\n",
      "Train Epoch: 1 [27520/48000 (57%)]\tLoss: 0.017235\n",
      "Train Epoch: 1 [28160/48000 (59%)]\tLoss: 0.002522\n",
      "Train Epoch: 1 [28800/48000 (60%)]\tLoss: 0.246625\n",
      "Train Epoch: 1 [29440/48000 (61%)]\tLoss: 0.174261\n",
      "Train Epoch: 1 [30080/48000 (63%)]\tLoss: 0.042226\n",
      "Train Epoch: 1 [30720/48000 (64%)]\tLoss: 0.120910\n",
      "Train Epoch: 1 [31360/48000 (65%)]\tLoss: 0.020426\n",
      "Train Epoch: 1 [32000/48000 (67%)]\tLoss: 0.018777\n",
      "Train Epoch: 1 [32640/48000 (68%)]\tLoss: 0.133221\n",
      "Train Epoch: 1 [33280/48000 (69%)]\tLoss: 0.098110\n",
      "Train Epoch: 1 [33920/48000 (71%)]\tLoss: 0.009995\n",
      "Train Epoch: 1 [34560/48000 (72%)]\tLoss: 0.032751\n",
      "Train Epoch: 1 [35200/48000 (73%)]\tLoss: 0.006117\n",
      "Train Epoch: 1 [35840/48000 (75%)]\tLoss: 0.042690\n",
      "Train Epoch: 1 [36480/48000 (76%)]\tLoss: 0.076330\n",
      "Train Epoch: 1 [37120/48000 (77%)]\tLoss: 0.155496\n",
      "Train Epoch: 1 [37760/48000 (79%)]\tLoss: 0.063101\n",
      "Train Epoch: 1 [38400/48000 (80%)]\tLoss: 0.028694\n",
      "Train Epoch: 1 [39040/48000 (81%)]\tLoss: 0.087714\n",
      "Train Epoch: 1 [39680/48000 (83%)]\tLoss: 0.038423\n",
      "Train Epoch: 1 [40320/48000 (84%)]\tLoss: 0.054956\n",
      "Train Epoch: 1 [40960/48000 (85%)]\tLoss: 0.059742\n",
      "Train Epoch: 1 [41600/48000 (87%)]\tLoss: 0.013987\n",
      "Train Epoch: 1 [42240/48000 (88%)]\tLoss: 0.049003\n",
      "Train Epoch: 1 [42880/48000 (89%)]\tLoss: 0.026259\n",
      "Train Epoch: 1 [43520/48000 (91%)]\tLoss: 0.021017\n",
      "Train Epoch: 1 [44160/48000 (92%)]\tLoss: 0.132868\n",
      "Train Epoch: 1 [44800/48000 (93%)]\tLoss: 0.099856\n",
      "Train Epoch: 1 [45440/48000 (95%)]\tLoss: 0.080438\n",
      "Train Epoch: 1 [46080/48000 (96%)]\tLoss: 0.002832\n",
      "Train Epoch: 1 [46720/48000 (97%)]\tLoss: 0.009238\n",
      "Train Epoch: 1 [47360/48000 (99%)]\tLoss: 0.116570\n",
      "\n",
      "Test/validation set: Average loss: 0.0004, Accuracy: 11619/12000 (97%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(num_epochs):\n",
    "    train(model, train_loader, loss_fn, i)\n",
    "    # Evaluate the model on the test set\n",
    "    test_model(model, val_loader, loss_fn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2dc8c7-acd8-4be7-9d9d-364ab8b51848",
   "metadata": {},
   "source": [
    "After learning we evaluate the model on the _test set_ and save the resulting structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "bc59f415-4c16-41f8-8ec9-a8c06dd5c6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Evaluate the model on the test set\n",
    "# test_model(model, test_loader, loss_fn)\n",
    "# # Save the model\n",
    "# save_model('conv.pt', model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c745b31-6631-4cbe-8bd8-860c35d0d565",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Exercises\n",
    "\n",
    "1. Familiarize yourself with the code above and consult the PyTorch documentation when needed.\n",
    "2. Experiment with different NN architectures (also varying the convolutional parameters: size, stride, padding, etc) and observe the effect wrt. the loss/accuracy on the training and validation dataset (training, validation). Note that when adding new layers (including dropout [Lecture 2, Slide 13], pooling, etc.) you need to make sure that the dimensionality of the layers match up. **IMPORTANT:** ignore the test set at this stage (i.e., comment out the relevant lines above) so that the results for the test set do not influence your model choice.\n",
    "3. In the model above we use a simple gradient descent learning scheme. Try other types of optimizers (see https://pytorch.org/docs/stable/optim.html) and analyze the effect.\n",
    "4. If you feel adventurous, try investigating some of the other datasets that come prepacakged with PyTorch (see https://pytorch.org/vision/0.8/datasets.html). For instnce, for FashionMNIST you only need to change the dataloader from datasets.MNIST to datasets.FashionMNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614a704f-b12b-487e-b0e2-8eb24c8d9081",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
